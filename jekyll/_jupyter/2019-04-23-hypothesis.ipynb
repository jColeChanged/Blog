{
  "cells": [
    {
      "cell_type": "raw",
      "source": "---\nlayout: post\ntitle: Toward Better Tests\npublished: true\n---\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% raw\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fkxKeiGT_4ka",
        "pycharm": {}
      },
      "source": "\u003e It is not only the violin that shapes the violinist, we are all\nshaped by the tools we train ourselves to use, and in this respect\nprogramming languages have a devious influence: they shape our\nthinking habits.\n\u003e\n\u003e Edsger W. Dijkstra\n\nIn order to do testing well, it\u0027s important to have tools that enable good practice.\nThis post uses Python to introduce stochastic property testing, but the general idea \nis applicable beyond Python and even beyond programming. I hope in reading this you \nreceive or are inspired to find an instrument which will help you to build more reliable \nsoftware [1][2]. \n\nWhat a test *does* is assert something about a program. Lets assume we \nhave a function that checks to see if the result of an addition function is always\npositive. While trying to do so, it could `assert` that the sum of `1` and `2` \nis greater than `0`.\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 876,
          "status": "ok",
          "timestamp": 1555030896809,
          "user": {
            "displayName": "Joshua Cole",
            "photoUrl": "https://lh3.googleusercontent.com/-azxup7Woe44/AAAAAAAAAAI/AAAAAAAAAfs/oqkqz6uDXqw/s64/photo.jpg",
            "userId": "00001493690827580996"
          },
          "user_tz": 420
        },
        "id": "24PM35zG_4kb",
        "outputId": "e613c09b-6cf9-47c9-ec9d-d382a07a7ff0",
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The test passed!\n"
          ]
        }
      ],
      "source": [
        "def add(a, b):\n",
        "    return a + b\n",
        "\n",
        "# Unparameterized test\n",
        "\n",
        "def test_sum_positive():\n",
        "   assert add(1, 2) \u003e 0\n",
        "\n",
        "try:\n",
        "    test_sum_positive()\n",
        "    print(\"The test passed!\")\n",
        "except AssertionError:\n",
        "    print(\"The test failed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O9jluLrJ_4kh",
        "pycharm": {}
      },
      "source": [
        "What a test *is* is a measurement of program execution under specific\n",
        "conditions. Since a test is a measurement, it follows that it is a [statistic](https://en.wikipedia.org/wiki/Statistic). So reallly, every set of tests is \n",
        "a sampling from the population of possible program executions.\n",
        "\n",
        "Seeing tests through the lens of statistics is useful, because one of the things \n",
        "statistics does is draw attention to the importance of good sampling. A hardcoded \n",
        "unit test might not seem like a problem, since any test at all is better than none.\n",
        "Statistics reminds us that \n",
        "[this type of sampling is flawed](https://en.wikipedia.org/wiki/Sampling_bias) \n",
        "such that we can\u0027t necessarily generalize from the sample population to the actual \n",
        "population. \n",
        "\n",
        "The measurement of `add(1, 2)` does not give much confidence that the addition \n",
        "function always returns positive results. The sampling of the add function isn\u0027t \n",
        "even close to being representative of the population of possible program executions.\n",
        "\n",
        "For the single addition test above, the test would pass. Despite this, there are\n",
        "many possible ways that the addition function could be called so as to fail to \n",
        "produce a positive number. The most obvious cases are when it\u0027s arguments aren\u0027t \n",
        "positive numbers, but there are others [3].\n",
        "\n",
        "One thing we know from statistics is that larger sample sizes are better than \n",
        "smaller sample sizes. Can we write our tests to include more samples, to help \n",
        "make our sample more represenative of all the possible ways the program could \n",
        "be executed? \n",
        "\n",
        "We can. It is possible to write a hundred variants of the add testing function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 1050,
          "status": "ok",
          "timestamp": 1555030896997,
          "user": {
            "displayName": "Joshua Cole",
            "photoUrl": "https://lh3.googleusercontent.com/-azxup7Woe44/AAAAAAAAAAI/AAAAAAAAAfs/oqkqz6uDXqw/s64/photo.jpg",
            "userId": "00001493690827580996"
          },
          "user_tz": 420
        },
        "id": "JE0Gc3HX_4ki",
        "outputId": "7cc1d6d8-69a2-404f-f1b4-451800d4b6a7",
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The tests passed!\n"
          ]
        }
      ],
      "source": [
        "def add(a, b):\n",
        "    return a + b\n",
        "\n",
        "# An example of a bunch of unparameterized tests\n",
        "\n",
        "skip \u003d 3\n",
        "a_few \u003d 99\n",
        "\n",
        "def test_sum_positive_1():\n",
        "   assert add(1, 2) \u003e 0\n",
        "\n",
        "def test_sum_positive_2():\n",
        "   assert add(skip, a_few) \u003e 0\n",
        "\n",
        "# ... snip\n",
        "\n",
        "def test_sum_positive_3():\n",
        "   assert add(99, 100) \u003e 0\n",
        "\n",
        "try:\n",
        "    test_sum_positive_1()\n",
        "    test_sum_positive_2()\n",
        "    # ... snip\n",
        "    test_sum_positive_3()\n",
        "    print(\"The tests passed!\")\n",
        "except AssertionError:\n",
        "    print(\"A test failed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Wnv96CW-_4kl",
        "pycharm": {}
      },
      "source": [
        "Even though we can do this, we shouldn\u0027t.\n",
        "\n",
        "Most functions take arguments. Even basic functions like the addition function\n",
        "can take arguments: the numbers to add together. Test functions can take\n",
        "arguments just like other functions can. Instead of hard coding `1` and `2` a\n",
        "test could be written which asserts that `a` plus `b` is positive.\n",
        "\n",
        "This type of test, a test which accepts arguments, is called a parameterized\n",
        "test. An advantage that a parameterized test has over an unparameterized test\n",
        "is that by passing a parameterized test different sets of arguments, it\u0027s possible \n",
        "to more concisely measure our program execution properties."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 1037,
          "status": "ok",
          "timestamp": 1555030896997,
          "user": {
            "displayName": "Joshua Cole",
            "photoUrl": "https://lh3.googleusercontent.com/-azxup7Woe44/AAAAAAAAAAI/AAAAAAAAAfs/oqkqz6uDXqw/s64/photo.jpg",
            "userId": "00001493690827580996"
          },
          "user_tz": 420
        },
        "id": "vzcOcAdq_4km",
        "outputId": "94dbdf49-077f-49ce-ebda-f19ab14f25a6",
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The tests passed!\n"
          ]
        }
      ],
      "source": [
        "def add(a, b):\n",
        "    return a + b\n",
        "\n",
        "skip \u003d 3\n",
        "a_few \u003d 99\n",
        "\n",
        "# An example of a bunch of tests, using paramaterized tests\n",
        "\n",
        "def test_paramaterized_sum_positive(a, b):\n",
        "    assert add(a, b) \u003e 0\n",
        "\n",
        "def paramaterized_test_runner():   \n",
        "    test_paramaterized_sum_positive(1, 2)\n",
        "    test_paramaterized_sum_positive(skip, a_few)\n",
        "    # ... snip\n",
        "    test_paramaterized_sum_positive(99, 100)\n",
        "\n",
        "try:\n",
        "    paramaterized_test_runner()\n",
        "    print(\"The tests passed!\")\n",
        "except AssertionError:\n",
        "    print(\"A test failed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ftuDomP__4kp",
        "pycharm": {}
      },
      "source": [
        "In the case of the sum testing function, this approach only saves about a line per\n",
        "measurement, because the test is relatively small. There are many tests which\n",
        "are longer than this. So consider a testing function that had ten lines of\n",
        "code, instead of only two.\n",
        "\n",
        "With an unparameterized test, if we were to take one hundred samples from the\n",
        "space of possible program executions, we would need to introduce a thousand\n",
        "lines worth of potential errors. Meanwhile, in the case of parameterized tests,\n",
        "we only need to risk about a hundred and ten lines worth of potential\n",
        "errors. This savings of nearly nine hundred potential sources of error is\n",
        "considerable. By this metric, parameterized tests are exceedingly better than \n",
        "unparameterized tests.\n",
        "\n",
        "| Function Length | Argument Length  | # Tests | Lines for Paramterized Tests | Lines for Unparamterized Tests|\n",
        "|-----------------|------------------|---------|------------------------------|-------------------------------|\n",
        "| 2               | 1                | 100     | 102                          | 200                           |\n",
        "| 10              | 1                | 100     | 110                          | 1000                          |\n",
        "\n",
        "Parameterized tests are supported in most\n",
        "popular testing frameworks. For example, `pytest` provides a higher-order\n",
        "function that accepts argument sets and a parameterized test and returns a test\n",
        "which tests every test variation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SEZZZdkI_4kp",
        "pycharm": {}
      },
      "source": [
        "# Installing Pytest\n",
        "\n",
        "Before I can show what parameterized tests look like in `pytest`, we need to install `pytest` and set it up \n",
        "to work in an ipython notebook.\n",
        "\n",
        "1. Find the Python installation we\u0027re using to run Jupyter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 1024,
          "status": "ok",
          "timestamp": 1555030896999,
          "user": {
            "displayName": "Joshua Cole",
            "photoUrl": "https://lh3.googleusercontent.com/-azxup7Woe44/AAAAAAAAAAI/AAAAAAAAAfs/oqkqz6uDXqw/s64/photo.jpg",
            "userId": "00001493690827580996"
          },
          "user_tz": 420
        },
        "id": "4y4lB6w9_4kq",
        "outputId": "c87d473d-e797-4c71-a0ce-59013b2c8bda",
        "pycharm": {}
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\u0027/usr/bin/python3\u0027"
            ]
          },
          "execution_count": 4,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import sys\n",
        "sys.executable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "81y-Cq_V_4kt",
        "pycharm": {}
      },
      "source": [
        "2. Install the dependencies we need into the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 4357,
          "status": "ok",
          "timestamp": 1555030900343,
          "user": {
            "displayName": "Joshua Cole",
            "photoUrl": "https://lh3.googleusercontent.com/-azxup7Woe44/AAAAAAAAAAI/AAAAAAAAAfs/oqkqz6uDXqw/s64/photo.jpg",
            "userId": "00001493690827580996"
          },
          "user_tz": 420
        },
        "id": "zEEkCDAd_4ku",
        "outputId": "fde80f71-c24e-4465-9847-cf0b76d52568",
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (3.6.4)\n",
            "Requirement already satisfied: ipython-pytest in /usr/local/lib/python3.6/dist-packages (0.0.1)\n",
            "Requirement already satisfied: atomicwrites\u003e\u003d1.0 in /usr/local/lib/python3.6/dist-packages (from pytest) (1.3.0)\n",
            "Requirement already satisfied: attrs\u003e\u003d17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest) (19.1.0)\n",
            "Requirement already satisfied: pluggy\u003c0.8,\u003e\u003d0.5 in /usr/local/lib/python3.6/dist-packages (from pytest) (0.7.1)\n",
            "Requirement already satisfied: more-itertools\u003e\u003d4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest) (7.0.0)\n",
            "Requirement already satisfied: six\u003e\u003d1.10.0 in /usr/local/lib/python3.6/dist-packages (from pytest) (1.11.0)\n",
            "Requirement already satisfied: py\u003e\u003d1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest) (1.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest) (40.9.0)\n"
          ]
        }
      ],
      "source": [
        "! /usr/bin/python3 -m pip install pytest ipython-pytest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JDdJcq_e_4ky",
        "pycharm": {}
      },
      "source": [
        "3. Tell ipython to load the `ipython_pytest` extension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "TvJqWsyk_4kz",
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "%load_ext ipython_pytest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Wjjh1RuD_4k2",
        "pycharm": {}
      },
      "source": [
        "Now with `pytest` installed, we can use it to write some parameterized tests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 6130,
          "status": "ok",
          "timestamp": 1555030902140,
          "user": {
            "displayName": "Joshua Cole",
            "photoUrl": "https://lh3.googleusercontent.com/-azxup7Woe44/AAAAAAAAAAI/AAAAAAAAAfs/oqkqz6uDXqw/s64/photo.jpg",
            "userId": "00001493690827580996"
          },
          "user_tz": 420
        },
        "id": "8diqDIF6_4k2",
        "outputId": "59654f46-98da-43ce-a1fd-5debcd2e8981",
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d test session starts \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n",
            "platform linux -- Python 3.6.7, pytest-3.6.4, py-1.8.0, pluggy-0.7.1\n",
            "hypothesis profile \u0027default\u0027 -\u003e database\u003dDirectoryBasedExampleDatabase(\u0027/tmp/tmpuliw3v4m/.hypothesis/examples\u0027)\n",
            "rootdir: /tmp/tmpuliw3v4m, inifile:\n",
            "plugins: hypothesis-4.15.0\n",
            "collected 3 items\n",
            "\n",
            "_ipytesttmp.py ...                                                       [100%]\n",
            "\n",
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d 3 passed in 0.09 seconds \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n"
          ]
        }
      ],
      "source": [
        "%%pytest\n",
        "\n",
        "# Parameterized test, with pytest\n",
        "import pytest\n",
        "\n",
        "\n",
        "def add(a, b):\n",
        "    return a + b\n",
        "\n",
        "skip \u003d 3\n",
        "a_few \u003d 99\n",
        "\n",
        "\n",
        "@pytest.mark.parametrize(\u0027a, b\u0027, [\n",
        "  (1, 2),\n",
        "  (skip, a_few),\n",
        "  # ...\n",
        "  (99, 100),\n",
        "])\n",
        "def test_sum_positive(a, b):\n",
        "   assert add(a, b) \u003e 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "66ExMFXO_4k5",
        "pycharm": {}
      },
      "source": [
        "Just with this, we\u0027re already in a much better place than we were. We\u0027re now testing multiple different parameterizations of our program. However, one of the only reasons this seems reasonable is that all this time the test specification has been skipping large numbers of parameterizations in the interest of brevity. In a real program, no one wants to have to type out a hundred different hard coded test cases. What would be much better is to both have many different parameterizations and to also have brevity at the same time.\n",
        "\n",
        "One way to do that is to have our arguments be programmatically generated rather than specified by hand."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 6880,
          "status": "ok",
          "timestamp": 1555030902899,
          "user": {
            "displayName": "Joshua Cole",
            "photoUrl": "https://lh3.googleusercontent.com/-azxup7Woe44/AAAAAAAAAAI/AAAAAAAAAfs/oqkqz6uDXqw/s64/photo.jpg",
            "userId": "00001493690827580996"
          },
          "user_tz": 420
        },
        "id": "Wm31aaks_4k6",
        "outputId": "1fc22038-f1da-4d46-af8e-78ed85717024",
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d test session starts \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n",
            "platform linux -- Python 3.6.7, pytest-3.6.4, py-1.8.0, pluggy-0.7.1\n",
            "hypothesis profile \u0027default\u0027 -\u003e database\u003dDirectoryBasedExampleDatabase(\u0027/tmp/tmpuliw3v4m/.hypothesis/examples\u0027)\n",
            "rootdir: /tmp/tmp893idtid, inifile:\n",
            "plugins: hypothesis-4.15.0\n",
            "collected 98 items\n",
            "\n",
            "_ipytesttmp.py ......................................................... [ 58%]\n",
            ".........................................                                [100%]\n",
            "\n",
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d warnings summary \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n",
            "\u003cundetermined location\u003e\n",
            "  Module already imported so cannot be rewritten: hypothesis\n",
            "\n",
            "-- Docs: http://doc.pytest.org/en/latest/warnings.html\n",
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d 98 passed, 1 warnings in 0.57 seconds \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n"
          ]
        }
      ],
      "source": [
        "%%pytest\n",
        "\n",
        "# Parameterized test, with pytest\n",
        "import pytest\n",
        "\n",
        "\n",
        "def add(a, b):\n",
        "    return a + b\n",
        "\n",
        "def create_sum_arguments(start, end):\n",
        "   \"Generate a list of test parameterizations.\"\n",
        "   return [(a, a+1) for a in range(start, end)]\n",
        "\n",
        "@pytest.mark.parametrize(\u0027a, b\u0027, create_sum_arguments(1, 99))\n",
        "def test_sum_positive(a, b):\n",
        "   assert add(a, b) \u003e 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "U1z467Y9_4k8",
        "pycharm": {}
      },
      "source": [
        "Let\u0027s take a moment to [compare](https://en.wikipedia.org/wiki/Big_O_notation) \n",
        "generating a list of argument with our previous approach of writing \n",
        "out each argument individually in terms of how many lines it takes to \n",
        "write the tests.\n",
        "\n",
        "|   Test Paramterizations       | Lines for hand specification | Lines for programmatic creation\n",
        "|-------------------------------|------------------------------|----------------\n",
        "|1                              | 1                            | 2\n",
        "|2                              | 2                            | 2\n",
        "|3                              | 3                            | 2\n",
        "|100                            | 100                          | 2\n",
        "|1000                           | 1000                         | 2\n",
        "|10000                          | 10000                        | 2\n",
        "|n                              | O(n)                         | O(1)\n",
        "\n",
        "\n",
        "With list creation, we can list a hundred different \n",
        "arguments in roughly two lines. With hand specification, it takes around a hundred lines.\n",
        "So the number of lines of code per test with list creation grows O(1) while the number \n",
        "of lines of code per test with hand specification grows O(n) where n is the number of test cases.\n",
        "\n",
        "This is a great improvement, especially if the constant size of each argument is high, but \n",
        "even with the add function **generating a million test paramterizations programmatically is \n",
        "cheaper in terms of hand movement than writing three manually**.\n",
        "\n",
        "With that power in mind, one weakness of creating a list of test cases is the amount of memory used. It isn\u0027t obvious \n",
        "with the add function, but consider the case of testing an image parsing library\n",
        "with each test case being an image buffer of a 1 MB image.\n",
        "\n",
        "Although we are able to generate \n",
        "our test cases in O(1) lines of code, it is still going to consume O(n) memory where n is the \n",
        "number of images. If we generated a hundred samples, we would need 100 MB of space. With a \n",
        "million, we\u0027re likely to run out of memory and crash.\n",
        "\n",
        "It is very easy to fix that, by taking advantage of the [lazy evaluation](https://en.wikipedia.org/wiki/Lazy_evaluation) via a [generator expression](https://www.python.org/dev/peps/pep-0289/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 7495,
          "status": "ok",
          "timestamp": 1555030903519,
          "user": {
            "displayName": "Joshua Cole",
            "photoUrl": "https://lh3.googleusercontent.com/-azxup7Woe44/AAAAAAAAAAI/AAAAAAAAAfs/oqkqz6uDXqw/s64/photo.jpg",
            "userId": "00001493690827580996"
          },
          "user_tz": 420
        },
        "id": "bn6MvVrx_4k8",
        "outputId": "2add54d0-d6a7-43b8-ea64-6b11644e9602",
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d test session starts \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n",
            "platform linux -- Python 3.6.7, pytest-3.6.4, py-1.8.0, pluggy-0.7.1\n",
            "hypothesis profile \u0027default\u0027 -\u003e database\u003dDirectoryBasedExampleDatabase(\u0027/tmp/tmpuliw3v4m/.hypothesis/examples\u0027)\n",
            "rootdir: /tmp/tmpzpc558np, inifile:\n",
            "plugins: hypothesis-4.15.0\n",
            "collected 98 items\n",
            "\n",
            "_ipytesttmp.py ......................................................... [ 58%]\n",
            ".........................................                                [100%]\n",
            "\n",
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d warnings summary \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n",
            "\u003cundetermined location\u003e\n",
            "  Module already imported so cannot be rewritten: hypothesis\n",
            "\n",
            "-- Docs: http://doc.pytest.org/en/latest/warnings.html\n",
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d 98 passed, 1 warnings in 0.56 seconds \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n"
          ]
        }
      ],
      "source": [
        "%%pytest\n",
        "\n",
        "# Parameterized test, with pytest\n",
        "import pytest\n",
        "\n",
        "\n",
        "def add(a, b):\n",
        "    return a + b\n",
        "\n",
        "def yield_sum_arguments(start, end):\n",
        "   \"Return a generator which yields test parameterizations tuples.\"\n",
        "   return ((a, a+1) for a in range(start, end))\n",
        "\n",
        "@pytest.mark.parametrize(\u0027a, b\u0027, yield_sum_arguments(1, 99))\n",
        "def test_sum_positive(a, b):\n",
        "   assert add(a, b) \u003e 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1ezr_LNb_4k_",
        "pycharm": {}
      },
      "source": [
        "|   Test Paramterizations       | Memory used for list creation | Memory used in lazy evaluation\n",
        "|-------------------------------|-------------------------------|----------\n",
        "|1                              | 1 MB                          | 1 MB\n",
        "|2                              | 2 MB                          | 1 MB\n",
        "|3                              | 3 MB                          | 1 MB\n",
        "|100                            | 100 MB                        | 1 MB\n",
        "|1000                           | 1000 MB                       | 1 MB\n",
        "|10000                          | 10000 MB                      | 1 MB\n",
        "|n                              | O(n)                          | O(1)\n",
        "\n",
        "\n",
        "\n",
        "Lazy evaluation helps with more than our space issue. It also improves the \n",
        "development cycle of an engineer interacting with the tests. When fixing breaking \n",
        "test cases, we\u0027re not interested in every test. Having to do a lot of work \n",
        "generating test cases that will never run is a waste of time.\n",
        "\n",
        "It\u0027s a good sign that as we improve the architecture of our tests, we start \n",
        "getting more and more useful properties. Parameterized tests which are generated \n",
        "as needed are a step in the right direction.\n",
        "\n",
        "Still, so far none of the tests have managed to show us the obvious - calling \n",
        "add on two negative numbers isn\u0027t going to return a positive number.\n",
        "\n",
        "Really, we don\u0027t want to write a loop that generates parameters so much as a loop that\n",
        "generates a set of parameters which finds places where our hypothesis about how\n",
        "our program works is not true.\n",
        "\n",
        "We want something like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ugRUrzfs_4lA",
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "# @pytest.mark.parametrize(\u0027a, b\u0027, a_scientific_search_strategy)\n",
        "# def test_sum_positive(a, b):\n",
        "#   assert add(a, b) \u003e 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1ZCsepy6_4lB",
        "pycharm": {}
      },
      "source": [
        "But how do we get that?\n",
        "\n",
        "Back in the 1990s Koen Claessen and John Hughes\n",
        "wrote a paper on QuickCheck, a Haskell testing library which aided Haskell\n",
        "programmers in formulating and testing the properties of programs [4]. The\n",
        "library gives tools to make writing these sort of parameterized tests easy. It\n",
        "even goes farther than that and tries to simplify any samples it finds which\n",
        "cause a failure, so that the failures it finds are easier for an engineer to\n",
        "understand.\n",
        "\n",
        "In the world of people who care about testing, this library has been influential.\n",
        "It\u0027s been [ported to several other languages](https://hypothesis.works/articles/quickcheck-in-every-language/), with varying degrees of rigour. For some languages, it\u0027s even been elevated \n",
        "into core testing libraries.\n",
        "\n",
        "Here is the instrument that I hope to give you.\n",
        "It is a testing library, inspired by QuickCheck, called `hypothesis`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "n5yOa635_4lC",
        "pycharm": {}
      },
      "source": [
        "#### What is hypothesis?\n",
        "\n",
        "\u003e Hypothesis is a modern implementation of property based testing.\n",
        "\u003e\n",
        "\u003e Hypothesis runs your tests against a much wider range of scenarios than a human\n",
        "tester could, finding edge cases in your code that you would otherwise have\n",
        "missed. It then turns them into simple and easy to understand failures that\n",
        "save you time and money compared to fixing them if they slipped through the\n",
        "cracks and a user had run into them instead.\n",
        "\u003e\n",
        "\u003e Hypothesis integrates into your normal testing workflow. Getting started is as\n",
        "simple as installing a library and writing some code using it - no new services\n",
        "to run, no new test runners to learn.\n",
        "\u003e\n",
        "\u003e https://hypothesis.works\n",
        "\n",
        "#### What is hypothesis for?\n",
        "\n",
        "\u003e From the perspective of a user, the purpose of Hypothesis is to make it\n",
        "\u003e easier for you to write better tests.\n",
        "\u003e\n",
        "\u003e From my perspective as the author, that is of course also a purpose of\n",
        "\u003e Hypothesis, but (if you will permit me to indulge in a touch of megalomania\n",
        "\u003e for a moment), the larger purpose of Hypothesis is to drag the world kicking\n",
        "\u003e and screaming into a new and terrifying age of high quality software.\n",
        "\u003e\n",
        "\u003e Software is, as they say, eating the world. Software is also terrible. It’s\n",
        "\u003e buggy, insecure and generally poorly thought out. This combination is clearly\n",
        "\u003e a recipe for disaster.\n",
        "\u003e\n",
        "\u003e And the state of software testing is even worse. Although it’s fairly\n",
        "\u003e uncontroversial at this point that you should be testing your code, can you\n",
        "\u003e really say with a straight face that most projects you’ve worked on are\n",
        "\u003e adequately tested?\n",
        "\u003e\n",
        "\u003e A lot of the problem here is that it’s too hard to write good tests. Your\n",
        "\u003e tests encode exactly the same assumptions and fallacies that you had when you\n",
        "\u003e wrote the code, so they miss exactly the same bugs that you missed when you\n",
        "\u003e wrote the code.\n",
        "\u003e\n",
        "\u003e Meanwhile, there are all sorts of tools for making testing better that are\n",
        "\u003e basically unused. The original Quickcheck is from 1999 and the majority of\n",
        "\u003e developers have not even heard of it, let alone used it. There are a bunch of\n",
        "\u003e half-baked implementations for most languages, but very few of them are worth\n",
        "\u003e using.\n",
        "\u003e\n",
        "\u003e ...\n",
        "\u003e\n",
        "\u003e https://hypothesis.readthedocs.io/en/latest/manifesto.html\n",
        "\n",
        "## Installing Hypothesis\n",
        "\n",
        "Let\u0027s begin with hypothesis where we left off with a paramterized testing strategy. To do that, we will first need to install it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 10538,
          "status": "ok",
          "timestamp": 1555030906578,
          "user": {
            "displayName": "Joshua Cole",
            "photoUrl": "https://lh3.googleusercontent.com/-azxup7Woe44/AAAAAAAAAAI/AAAAAAAAAfs/oqkqz6uDXqw/s64/photo.jpg",
            "userId": "00001493690827580996"
          },
          "user_tz": 420
        },
        "id": "AlqcndQt_4lD",
        "outputId": "46342da3-c2a5-46e4-b0bc-d6f99f93e2e1",
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: hypothesis in /usr/local/lib/python3.6/dist-packages (4.15.0)\n",
            "Requirement already satisfied: attrs\u003e\u003d16.0.0 in /usr/local/lib/python3.6/dist-packages (from hypothesis) (19.1.0)\n"
          ]
        }
      ],
      "source": [
        "! /usr/bin/python3 -m pip install hypothesis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "H8643yDN_4lH",
        "pycharm": {}
      },
      "source": [
        "Now that we\u0027ve installed it, we can write out our more scientific search strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 11156,
          "status": "ok",
          "timestamp": 1555030907203,
          "user": {
            "displayName": "Joshua Cole",
            "photoUrl": "https://lh3.googleusercontent.com/-azxup7Woe44/AAAAAAAAAAI/AAAAAAAAAfs/oqkqz6uDXqw/s64/photo.jpg",
            "userId": "00001493690827580996"
          },
          "user_tz": 420
        },
        "id": "V7Z4vkQT_4lI",
        "outputId": "abe38fb4-5fc1-432c-e22a-c27e7c110887",
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d test session starts \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n",
            "platform linux -- Python 3.6.7, pytest-3.6.4, py-1.8.0, pluggy-0.7.1\n",
            "hypothesis profile \u0027default\u0027 -\u003e database\u003dDirectoryBasedExampleDatabase(\u0027/tmp/tmpuliw3v4m/.hypothesis/examples\u0027)\n",
            "rootdir: /tmp/tmpxykp99zo, inifile:\n",
            "plugins: hypothesis-4.15.0\n",
            "collected 1 item\n",
            "\n",
            "_ipytesttmp.py F                                                         [100%]\n",
            "\n",
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d FAILURES \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n",
            "______________________________ test_sum_positive _______________________________\n",
            "\n",
            "    @given(strategies.integers(), strategies.integers())\n",
            "\u003e   def test_sum_positive(a, b):\n",
            "\n",
            "_ipytesttmp.py:10: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "a \u003d 0, b \u003d 0\n",
            "\n",
            "    @given(strategies.integers(), strategies.integers())\n",
            "    def test_sum_positive(a, b):\n",
            "\u003e      assert add(a, b) \u003e 0\n",
            "E      assert 0 \u003e 0\n",
            "E       +  where 0 \u003d add(0, 0)\n",
            "\n",
            "_ipytesttmp.py:11: AssertionError\n",
            "---------------------------------- Hypothesis ----------------------------------\n",
            "Falsifying example: test_sum_positive(a\u003d0, b\u003d0)\n",
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d warnings summary \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n",
            "\u003cundetermined location\u003e\n",
            "  Module already imported so cannot be rewritten: hypothesis\n",
            "\n",
            "-- Docs: http://doc.pytest.org/en/latest/warnings.html\n",
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d 1 failed, 1 warnings in 0.33 seconds \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n"
          ]
        }
      ],
      "source": [
        "%%pytest\n",
        "# Parameterized test, with pytest and hypothesis\n",
        "from hypothesis import given, strategies\n",
        "\n",
        "\n",
        "def add(a, b):\n",
        "    return a + b\n",
        "\n",
        "\n",
        "@given(strategies.integers(), strategies.integers())\n",
        "def test_sum_positive(a, b):\n",
        "   assert add(a, b) \u003e 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_o3f3bn8_4lL",
        "pycharm": {}
      },
      "source": [
        "This simple, unassuming bit of code is doing a lot. It generates a hundred\n",
        "samples; runs a hundred tests. Yet it does this with less code than our\n",
        "earlier parameterization approach and not only that, but for the first time we\n",
        "have code which will quickly find the error that is so blatant! It discovers the \n",
        "counterexamples where `a` and `b` are not positive.\n",
        "\n",
        "Let\u0027s break this down. `@given` is a [decorator](https://www.python.org/dev/peps/pep-0318/). \n",
        "A decorator is Python syntactic sugar for a [higher-order function](https://en.wikipedia.org/wiki/Higher-order_function), \n",
        "which is a function that accepts a function as an argument and returns another function. \n",
        "The `given` decorator takes in a data generation strategies and a parameterized test and\n",
        "returns a test which will call the parameterized test using the provided data\n",
        "generation strategy.\n",
        "\n",
        "`strategies` is a module imported from the `hypothesis` library. It contains\n",
        "functions which help an engineer to create samples from many common data types.\n",
        "It also provides ways to combine and compose strategies, so that even very\n",
        "complex data generation is possible.\n",
        "\n",
        "`integers` is one of the strategies it provides. It has\n",
        "a few arguments it accept which can be used to change what \n",
        "integers it will generate during testing, but I won\u0027t go into that here. The hypothesis project\n",
        "has [excellent documentation](https://hypothesis.readthedocs.io/en/latest/data.html).\n",
        "You can read about `integers()` and other data generation strategies there. For\n",
        "our purposes, it\u0027s more important to to know that there is a way to generate a lot of\n",
        "different data. Even complex data.\n",
        "\n",
        "Earlier I showed how to generate integers. We could easily extend this to also\n",
        "generate floats, decimals, fractions, and complex numbers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 11334,
          "status": "ok",
          "timestamp": 1555030907390,
          "user": {
            "displayName": "Joshua Cole",
            "photoUrl": "https://lh3.googleusercontent.com/-azxup7Woe44/AAAAAAAAAAI/AAAAAAAAAfs/oqkqz6uDXqw/s64/photo.jpg",
            "userId": "00001493690827580996"
          },
          "user_tz": 420
        },
        "id": "diLyYkKJ_4lM",
        "outputId": "64ffdacf-d83a-4d17-d2a2-7b0696f6063c",
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d test session starts \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n",
            "platform linux -- Python 3.6.7, pytest-3.6.4, py-1.8.0, pluggy-0.7.1\n",
            "hypothesis profile \u0027default\u0027 -\u003e database\u003dDirectoryBasedExampleDatabase(\u0027/tmp/tmpuliw3v4m/.hypothesis/examples\u0027)\n",
            "rootdir: /tmp/tmpiq6lujss, inifile:\n",
            "plugins: hypothesis-4.15.0\n",
            "collected 1 item\n",
            "\n",
            "_ipytesttmp.py F                                                         [100%]\n",
            "\n",
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d FAILURES \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n",
            "______________________________ test_sum_positive _______________________________\n",
            "\n",
            "    @given(number_strategy, number_strategy)\n",
            "\u003e   def test_sum_positive(a, b):\n",
            "\n",
            "_ipytesttmp.py:18: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "a \u003d 0.0, b \u003d 0.0\n",
            "\n",
            "    @given(number_strategy, number_strategy)\n",
            "    def test_sum_positive(a, b):\n",
            "\u003e      assert add(a, b) \u003e 0\n",
            "E      assert 0.0 \u003e 0\n",
            "E       +  where 0.0 \u003d add(0.0, 0.0)\n",
            "\n",
            "_ipytesttmp.py:19: AssertionError\n",
            "---------------------------------- Hypothesis ----------------------------------\n",
            "Falsifying example: test_sum_positive(a\u003d0.0, b\u003d0.0)\n",
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d warnings summary \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n",
            "\u003cundetermined location\u003e\n",
            "  Module already imported so cannot be rewritten: hypothesis\n",
            "\n",
            "-- Docs: http://doc.pytest.org/en/latest/warnings.html\n",
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d 1 failed, 1 warnings in 0.18 seconds \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n"
          ]
        }
      ],
      "source": [
        "%%pytest\n",
        "# More complex data generation strategies\n",
        "\n",
        "from hypothesis import given, strategies\n",
        "\n",
        "\n",
        "def add(a, b):\n",
        "    return a + b\n",
        "\n",
        "number_strategy \u003d (\n",
        "  strategies.floats() |\n",
        "  strategies.decimals() |\n",
        "  strategies.complex_numbers() |\n",
        "  strategies.fractions() |\n",
        "  strategies.integers() \n",
        ")\n",
        "\n",
        "@given(number_strategy, number_strategy)\n",
        "def test_sum_positive(a, b):\n",
        "   assert add(a, b) \u003e 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ijdz8jgK_4lS",
        "pycharm": {}
      },
      "source": [
        "Hypothesis gives a lot of expressive power. It was very easy to get a data \n",
        "generator that generated a much more rich set of data for testing. \n",
        "\n",
        "For the purpose of driving this point home, let\u0027s suppose that for some reason \n",
        "our addition function wasn\u0027t supposed to ever be called with `0` as an argument \n",
        "or with a number that somehow became a `NaN`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 11925,
          "status": "ok",
          "timestamp": 1555030907998,
          "user": {
            "displayName": "Joshua Cole",
            "photoUrl": "https://lh3.googleusercontent.com/-azxup7Woe44/AAAAAAAAAAI/AAAAAAAAAfs/oqkqz6uDXqw/s64/photo.jpg",
            "userId": "00001493690827580996"
          },
          "user_tz": 420
        },
        "id": "UW-miFRs_4lT",
        "outputId": "61e88817-517f-45d7-fde1-e87f84aebe89",
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d test session starts \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n",
            "platform linux -- Python 3.6.7, pytest-3.6.4, py-1.8.0, pluggy-0.7.1\n",
            "hypothesis profile \u0027default\u0027 -\u003e database\u003dDirectoryBasedExampleDatabase(\u0027/tmp/tmpuliw3v4m/.hypothesis/examples\u0027)\n",
            "rootdir: /tmp/tmpdxsnwtw3, inifile:\n",
            "plugins: hypothesis-4.15.0\n",
            "collected 1 item\n",
            "\n",
            "_ipytesttmp.py F                                                         [100%]\n",
            "\n",
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d FAILURES \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n",
            "______________________________ test_sum_positive _______________________________\n",
            "\n",
            "    @given(number_strategy, number_strategy)\n",
            "\u003e   def test_sum_positive(a, b):\n",
            "\n",
            "_ipytesttmp.py:37: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "a \u003d 1, b \u003d -1\n",
            "\n",
            "    @given(number_strategy, number_strategy)\n",
            "    def test_sum_positive(a, b):\n",
            "        assume(a !\u003d 0 and b !\u003d 0 and not math.isnan(a) and not math.isnan(b))\n",
            "\u003e       assert add(a, b) \u003e 0\n",
            "E       assert 0 \u003e 0\n",
            "E        +  where 0 \u003d add(1, -1)\n",
            "\n",
            "_ipytesttmp.py:39: AssertionError\n",
            "---------------------------------- Hypothesis ----------------------------------\n",
            "Falsifying example: test_sum_positive(a\u003d1, b\u003d-1)\n",
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d warnings summary \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n",
            "\u003cundetermined location\u003e\n",
            "  Module already imported so cannot be rewritten: hypothesis\n",
            "\n",
            "-- Docs: http://doc.pytest.org/en/latest/warnings.html\n",
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d 1 failed, 1 warnings in 0.42 seconds \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n"
          ]
        }
      ],
      "source": [
        "%%pytest\n",
        "# More complex data generation strategies\n",
        "import math\n",
        "\n",
        "\n",
        "from hypothesis import given, strategies\n",
        "\n",
        "\n",
        "def add(a, b):\n",
        "    return a + b\n",
        "\n",
        "number_strategy \u003d (\n",
        "  strategies.floats() |\n",
        "  strategies.decimals() |\n",
        "  strategies.complex_numbers() |\n",
        "  strategies.fractions() |\n",
        "  strategies.integers() \n",
        ").filter(lambda x: x !\u003d 0 and not math.isnan(x))\n",
        "\n",
        "@given(number_strategy, number_strategy)\n",
        "def test_sum_positive(a, b):\n",
        "    assert add(a, b) \u003e 0\n",
        "\n",
        "\n",
        "# Or we could use assume\n",
        "\n",
        "from hypothesis import assume\n",
        "\n",
        "number_strategy \u003d (\n",
        "  strategies.floats() |\n",
        "  strategies.decimals() |\n",
        "  strategies.complex_numbers() |\n",
        "  strategies.fractions() |\n",
        "  strategies.integers() \n",
        ")\n",
        "\n",
        "@given(number_strategy, number_strategy)\n",
        "def test_sum_positive(a, b):\n",
        "    assume(a !\u003d 0 and b !\u003d 0 and not math.isnan(a) and not math.isnan(b))\n",
        "    assert add(a, b) \u003e 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O8aG-Brm_4lW",
        "pycharm": {}
      },
      "source": [
        "If you\u0027ve been playing with these examples, you may have noticed that \n",
        "hypothesis has found breaking examples quite easily. Not just ones where the \n",
        "function returned a negative result, but actual errors. The add function isn\u0027t \n",
        "overloaded so as to support adding arbitray numeric types. If it wasn\u0027t an error \n",
        "you expected to see, than you may begin to understand just how useful this \n",
        "search for falsifying examples is. It doesn\u0027t just give you confidence that \n",
        "your code works. It can end up teaching you something you hadn\u0027t known.\n",
        "\n",
        "At the same time, you might not have seen these errors. When hypothesis runs \n",
        "tests, it generates them stoachastically. It\u0027s possible for two different runs \n",
        "of hypothesis to generate different examples.\n",
        "\n",
        "This is an important thing to keep in mind when using `hypothesis`. It is \n",
        "generating data, not doing magic. Under the hood it has a search strategy \n",
        "which is searching for things to pass in as test parameters. Given a very \n",
        "strict data generation strategy, it might not be able to find valid \n",
        "parameterizations. Even if it does find parameterizations, it could still \n",
        "skip over important test cases that an engineer knows to be of critical \n",
        "importance.\n",
        "\n",
        "For this reason and others, it can still be useful to hand specify important test cases. This can \n",
        "be done using the `@example` decorator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 12137,
          "status": "ok",
          "timestamp": 1555030908217,
          "user": {
            "displayName": "Joshua Cole",
            "photoUrl": "https://lh3.googleusercontent.com/-azxup7Woe44/AAAAAAAAAAI/AAAAAAAAAfs/oqkqz6uDXqw/s64/photo.jpg",
            "userId": "00001493690827580996"
          },
          "user_tz": 420
        },
        "id": "bPDGNGjf_4lX",
        "outputId": "7d80141a-fafc-4683-c28c-9a29d419bb3b",
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d test session starts \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n",
            "platform linux -- Python 3.6.7, pytest-3.6.4, py-1.8.0, pluggy-0.7.1\n",
            "hypothesis profile \u0027default\u0027 -\u003e database\u003dDirectoryBasedExampleDatabase(\u0027/tmp/tmpuliw3v4m/.hypothesis/examples\u0027)\n",
            "rootdir: /tmp/tmpp9g8cudf, inifile:\n",
            "plugins: hypothesis-4.15.0\n",
            "collected 1 item\n",
            "\n",
            "_ipytesttmp.py F                                                         [100%]\n",
            "\n",
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d FAILURES \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n",
            "______________________________ test_sum_positive _______________________________\n",
            "\n",
            "    @given(strategies.integers(), strategies.integers())\n",
            "\u003e   @example(-100, -100)\n",
            "    def test_sum_positive(a, b):\n",
            "\n",
            "_ipytesttmp.py:10: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "/usr/local/lib/python3.6/dist-packages/hypothesis/core.py:327: in execute_explicit_examples\n",
            "    test_runner(None, lambda data: test(*arguments, **example_kwargs))\n",
            "/usr/local/lib/python3.6/dist-packages/hypothesis/executors.py:56: in default_new_style_executor\n",
            "    return function(data)\n",
            "/usr/local/lib/python3.6/dist-packages/hypothesis/core.py:327: in \u003clambda\u003e\n",
            "    test_runner(None, lambda data: test(*arguments, **example_kwargs))\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "a \u003d -100, b \u003d -100\n",
            "\n",
            "    @given(strategies.integers(), strategies.integers())\n",
            "    @example(-100, -100)\n",
            "    def test_sum_positive(a, b):\n",
            "\u003e      assert add(a, b) \u003e 0\n",
            "E      assert -200 \u003e 0\n",
            "E       +  where -200 \u003d add(-100, -100)\n",
            "\n",
            "_ipytesttmp.py:12: AssertionError\n",
            "---------------------------------- Hypothesis ----------------------------------\n",
            "Falsifying example: test_sum_positive(a\u003d-100, b\u003d-100)\n",
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d warnings summary \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n",
            "\u003cundetermined location\u003e\n",
            "  Module already imported so cannot be rewritten: hypothesis\n",
            "\n",
            "-- Docs: http://doc.pytest.org/en/latest/warnings.html\n",
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d 1 failed, 1 warnings in 0.20 seconds \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n"
          ]
        }
      ],
      "source": [
        "%%pytest\n",
        "# Parameterized test, with pytest and hypothesis\n",
        "from hypothesis import given, strategies, example\n",
        "\n",
        "\n",
        "def add(a, b):\n",
        "    return a + b\n",
        "\n",
        "\n",
        "@given(strategies.integers(), strategies.integers())\n",
        "@example(-100, -100)\n",
        "def test_sum_positive(a, b):\n",
        "   assert add(a, b) \u003e 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VmDFnvM-_4lZ",
        "pycharm": {}
      },
      "source": [
        "#### Environmental Considerations\n",
        "\n",
        "There are concerns that a developer has which aren\u0027t about the beauty of an \n",
        "approach, but about the practicality of use. There are many programming \n",
        "languages that are objectively better than Python by one metric or another, yet it has it\u0027s niche, \n",
        "because it chooses to be readable rather than fast or easily parsed by computers. \n",
        "\n",
        "So far we\u0027ve discussed code. Let\u0027s move on to some other software engineering \n",
        "best practices and see how `hypothesis` holds up.\n",
        "\n",
        "Ironically, since we\u0027re using Python, when it comes to testing speed matters. \n",
        "There are things that can be done when something takes seconds\n",
        "which can\u0027t even be contemplated when something takes an hour. For example, \n",
        "having a continual testing loop running concurrently with development or \n",
        "running tests before check in become a much simpler thing when tests run \n",
        "quickly, but can slow down development speed if tests take hours to run.\n",
        "\n",
        "`hypothesis` does well in helping to deliver the ideal of fast running tests. \n",
        "It provides tools for limiting both the runtime of tests according to the clock \n",
        "and for limiting the runtime of tests in terms of the number of test cases it checks.\n",
        "\n",
        "On the topic of a contiual test loop, it\u0027s also nice to keep a record of what tests \n",
        "fail and than re-run those tests on the next run of the tester. `hypothesis` does \n",
        "this too.\n",
        "\n",
        "One way that these sorts of things can be done on a per test basis is through the settings\n",
        "decorator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 12860,
          "status": "ok",
          "timestamp": 1555030908950,
          "user": {
            "displayName": "Joshua Cole",
            "photoUrl": "https://lh3.googleusercontent.com/-azxup7Woe44/AAAAAAAAAAI/AAAAAAAAAfs/oqkqz6uDXqw/s64/photo.jpg",
            "userId": "00001493690827580996"
          },
          "user_tz": 420
        },
        "id": "WhcpgiIF_4lb",
        "outputId": "aee4d761-4db1-4733-b65f-0c657d5bf3be",
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d test session starts \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n",
            "platform linux -- Python 3.6.7, pytest-3.6.4, py-1.8.0, pluggy-0.7.1\n",
            "hypothesis profile \u0027default\u0027 -\u003e database\u003dDirectoryBasedExampleDatabase(\u0027/tmp/tmpuliw3v4m/.hypothesis/examples\u0027)\n",
            "rootdir: /tmp/tmp0nvvccac, inifile:\n",
            "plugins: hypothesis-4.15.0\n",
            "collected 3 items\n",
            "\n",
            "_ipytesttmp.py ...                                                       [100%]\n",
            "\n",
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d warnings summary \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n",
            "\u003cundetermined location\u003e\n",
            "  Module already imported so cannot be rewritten: hypothesis\n",
            "\n",
            "-- Docs: http://doc.pytest.org/en/latest/warnings.html\n",
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d 3 passed, 1 warnings in 0.80 seconds \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n"
          ]
        }
      ],
      "source": [
        "%%pytest\n",
        "\n",
        "from hypothesis import given, settings, strategies\n",
        "\n",
        "@given(strategies.integers())\n",
        "@settings(max_examples\u003d50) # Default is 100\n",
        "def test_this_a_little(x):\n",
        "    assert True\n",
        "    \n",
        "\n",
        "@given(strategies.integers())\n",
        "def test_this_with_default_setttings(x):\n",
        "    assert True\n",
        "    \n",
        "@given(strategies.integers())\n",
        "@settings(max_examples\u003d500)\n",
        "def test_this_thoroughly(x):\n",
        "    assert True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LdoA5FHL_4ld",
        "pycharm": {}
      },
      "source": [
        "Another way is via the settings profile:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 12845,
          "status": "ok",
          "timestamp": 1555030908951,
          "user": {
            "displayName": "Joshua Cole",
            "photoUrl": "https://lh3.googleusercontent.com/-azxup7Woe44/AAAAAAAAAAI/AAAAAAAAAfs/oqkqz6uDXqw/s64/photo.jpg",
            "userId": "00001493690827580996"
          },
          "user_tz": 420
        },
        "id": "TPSvW7DE_4le",
        "outputId": "ee6a8d69-d461-4a95-bdd5-882dd50c7a70",
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max examples before loading ci profile 100\n",
            "Max examples after loading ci profile 1000\n"
          ]
        }
      ],
      "source": [
        "from hypothesis import  settings, Verbosity\n",
        "\n",
        "settings.register_profile(\"ci\", max_examples\u003d1000)\n",
        "print(\"Max examples before loading ci profile\", settings().max_examples)\n",
        "settings.load_profile(\"ci\")\n",
        "print(\"Max examples after loading ci profile\", settings().max_examples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "v1ZmX16R_4lg",
        "pycharm": {}
      },
      "source": [
        "Beyond just running fast, thorough tests are also a nice to thing to have. \n",
        "Thorough testing gives more confidence that software works as desired than \n",
        "minimal testing, all other factors ignored.\n",
        "\n",
        "Since the settings profiles can be set based on flags, `hypothesis` tackles \n",
        "supporting more thorough testing in automated build servers in a straightforward \n",
        "way. When doing local development, we can run just a few tests. Meanwhile, \n",
        "on a build server we can run thousands."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "UeADqCAE_4lg",
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from hypothesis import settings, Verbosity\n",
        "settings.register_profile(\"weekend\", max_examples\u003d100000)\n",
        "settings.register_profile(\"nightly\", max_examples\u003d10000)\n",
        "settings.register_profile(\"ci\", max_examples\u003d1000)\n",
        "settings.register_profile(\"dev\", max_examples\u003d100)\n",
        "settings.register_profile(\"debug\", max_examples\u003d10, verbosity\u003dVerbosity.verbose)\n",
        "settings.load_profile(os.getenv(u\u0027HYPOTHESIS_PROFILE\u0027, \u0027default\u0027))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Og1xtUtK_4li",
        "pycharm": {}
      },
      "source": [
        "The `sympy` project found errors after letting `hypothesis` generate millions of\n",
        "examples. Errors that hadn\u0027t been found with less thorough testing. \n",
        "\n",
        "Hypothesis gives us an easy to tune knob which results in more thorough testing. \n",
        "This control gives a lot more power to engineers to make trade offs based on the \n",
        "context in which tests are being run. Out of band testing can be thorough to the point of\n",
        "absurdity while work that is in flow can be fast so as to maximize engineer\n",
        "productivity.\n",
        "\n",
        "#### Library integration\n",
        "\n",
        "Hypothesis has support for many popular python librarys, including django. It can infer strategy creation from a Django model or form. In a hypothetical example, let\u0027s say you have an email address model which is related to a contact model, which is related to an organization model. In order to get a strategy which could automatically fill in all the data for instances of those models all that would be needed would be:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ei_zo0sg_4lp",
        "pycharm": {}
      },
      "source": [
        "#### In Summary\n",
        "\n",
        "Do not kick. Do not scream. Cast aside the hard coded test cases and accept a\n",
        "better violin.\n",
        "\n",
        "[hypothesis.works](https://hypothesis.works/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oUSP_NwG_4lq",
        "pycharm": {}
      },
      "source": [
        "# Footnotes\n",
        "\n",
        "\\[1\\]: In his wonderful essay the [Humble Programmer](https://www.cs.utexas.edu/~EWD/transcriptions/EWD03xx/EWD340.html) Edsger W. Dijkstra goes on at length about limiting the scope of programs to those that can actually be contended with. In the same spirit of humility, I want to qualify my own words. Reliable software isn\u0027t easy to write. Great engineers and scientists have quipped that programming is the act of putting bugs into a program, since debugging is the act of taking them out. For this reason, when I write \"software\" I\u0027m defining it in a very narrow sense of the word; not full applications, but small sections of a program.\n",
        "\n",
        "I\u0027m also defining \"reliable\" in a narrow sense. When I say reliable, I mean that we have good reason to be confident that the software has the properties which we desire it to have. We expect the software to work how we think it works, rather than some other way. So reliable as in \"we think we might be able rely on it\", not, this definitely works always. I wish I could say that I was doing more than building confidence that the programs we write really did have the properties we hoped they would have, but the instrument I\u0027m sharing does not generate proofs. It is a way to generate tests.\n",
        "\n",
        "\\[2\\]: This goes beyond programming languages and testing libraries. For example, using Powerpoint can damage thinking. One humorous example of PowerPoint struggling to capture importance is the [Gettysburg address as converted to powerpoint by Peter Norvig](https://norvig.com/Gettysburg/). \n",
        "\n",
        "A more serious discussion of the weaknesses and corrupting biases of mediums can be found in the book [Amusing Ourselves to Death: Public Discourse in the Age of Show Business](https://www.amazon.com/Amusing-Ourselves-Death-Discourse-Business/dp/014303653X) by Neil Postman. In the book, Neil argues persuasively that mediums have an influence on the things that are written in them and that the influence of the medium of a long text document is more conductive to good thinking than a primarly visual medium. The critical take away form the book is that choosing the wrong medium doesn\u0027t just change how the content is presented, it changes the content itself. Given this, it shouldn\u0027t be much of a suprise that companies like Amazon are [forsaking PowerPoint in favor of the written word](https://www.inc.com/carmine-gallo/jeff-bezos-bans-powerpoint-in-meetings-his-replacement-is-brilliant.html).\n",
        "\n",
        "\\[3\\]: Beyond the normal errors there are other problems the add function is potentially \n",
        "vulnerable to. For example, we haven\u0027t verified that it is resistant to bit corruption \n",
        "by alpha particles or faulty hardware. Everything we leave out of our test is a \n",
        "potential source of a flaky test, since each uncontroller parameter is still an implicit \n",
        "parameters that can change program execution.\n",
        "\n",
        "In the case of the add function, this problem might feel theoretical, but it\u0027s not \n",
        "theoretical at all with remote APIs. Allowing tests to simulate failures is important.\n",
        "\n",
        "\\[4\\]: http://www.eecs.northwestern.edu/~robby/courses/395-495-2009-fall/quick.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Bhe8oERd_4lr",
        "pycharm": {}
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Copy of Testing with Hypothesis.ipynb",
      "provenance": [
        {
          "file_id": "197xpRo1cYGSm19r3K1mWCkywZk_ijp_q",
          "timestamp": 1555030052149
        }
      ],
      "version": "0.3.2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "stem_cell": {
      "cell_type": "raw",
      "source": "",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}