<!DOCTYPE html>
<html lang="en">
  <head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
  	<meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="author" content="Joshua Lowell Cole" />
    <link rel="stylesheet" href="/css/bootstrap.css" media="screen">
  	<link rel="stylesheet" href="/css/bootstrap-responsive.css" media="screen">
    <link rel="stylesheet" href="/css/blog.css" media="screen">
    
      <title>Joshua Cole: A Search Engine Study</title>
    
  	<script src="http://ajax.googleapis.com/ajax/libs/webfont/1/webfont.js"></script>
  	<script>
  	WebFont.load({
  		google: {
  			families: ['Droid Sans', 'Droid Serif', 'Droid Sans Mono']
      }
  	});
  	</script>
  </head>
  <body>
  	<div class="row-fluid">
      <div class="span4">
        <header class="media">
          <img class="pull-left img-circle media-object" src="http://www.gravatar.com/avatar/dfcd7f132a84516bb79ae96236e94501.png">
          <div class="media-body">
            <h4 class="media-heading">My name is <a href="/">Joshua Cole</a>...</h4>
            I'm a programmer, an aspiring statistician, and a Christian. I'm also into competitive gaming, especially League of Legends. My writing here tends to focus on things I'm learning about through self-study. If that interests you, enjoy!
          </div>
        </header>
      </div>
      <div class="span6">
		    <article>
  <h1><a href="/2013/07/03/a-search-engine-study.html">A Search Engine Study</a></h1>
  <h2 class="muted date"><small>03 Jul 2013</small></h2>
	<p>I just finished reading through chapter four of Programming Collective Intelligence. It went over crawlers, search engines, and neural networks. I ended up getting the chance to build an application which used each of these technologies.</p>

<p>Some of the technologies I had actually encountered before. I’m not new to scraping and crawler is just a scraper that ends up following links to continue it’s work. Having already been introduced to the topic before hand I actually found this section of the text to be rather disappointing. I wouldn’t trust the basics of the scraper that was given in this book, because it has a few glaring problems. An actual crawler needs to do things like respect robots.txt and handle rate limiting. It also might be wise to introduce caching so the same page doesn’t get scraped multiple times.</p>

<p>So when I wrote my scraper I did it a little differently the author did. In fact, I actually took a lot more inspiration from Peter Norvig than I did from Toby Segeran. In Paradigms of Artificial Intelligence Programming there was a beautiful method of treating a graph as a list that Peter ended up using. I recognized that the web was a graph, so I copied the central idea of that code.</p>

<p>The search engine itself was much more interesting to learn about. I learned how to make a toy search engine that ranked search results using some of the same scoring metrics that Google is known to have used.</p>

<p>I think the coolest thing I learned from this section was that its pretty easy to use multiple metrics at the same time. You can just sum up the metrics and sort as normal, as long as you’ve normalized the return values. If you want one thing to be more important then another all you need to do is add a weight to each metric. It becomes not too dissimilar to calculating GPA. You multiply how well it did by how much credit things were worth to get the total amount of credit.</p>

<p>I suppose this section isn’t complete unless I touch on the one famous algorithm that I implemented as a part of this chapter. Pagerank.</p>

<p>Pagerank is a famous algorithm developed by the founders of Google. It defines the rank of every page to be the chance that a user would have reached that page by randomly sttarting at an initial website and proceeding to follow links randomly until they decided to stop.</p>

<p>If you spend some time thinking about it, it should be pretty obvious that really popular sites are linked to frequently. So a person randomly clicking around is more likely to get to one of these websites then a less popular website. So that page would probably receive a high-score when computed using Google’s Pagerank algorithm. Meanwhile, the sites that the popular sites linked to would also tend to be fairly popular.</p>

<h1 id="what_i_did">What I Did</h1>

<ul>
<li>I built a crawler.</li>

<li>I built a search engine.</li>

<li>I implemented several scoring functions, including Pagerank.</li>

<li>I boggled at the neural network code. I’ll need to go back and re-read that section.</li>
</ul>
</article>
      </div>
    </div>
    <script type="text/javascript">/*<![CDATA[*/ window.olark||(function(k){var g=window,j=document,a=g.location.protocol=="https:"?"https:":"http:",i=k.name,b="load",h="addEventListener";(function(){g[i]=function(){(c.s=c.s||[]).push(arguments)};var c=g[i]._={},f=k.methods.length;while(f--){(function(l){g[i][l]=function(){g[i]("call",l,arguments)}})(k.methods[f])}c.l=k.loader;c.i=arguments.callee;c.p={0:+new Date};c.P=function(l){c.p[l]=new Date-c.p[0]};function e(){c.P(b);g[i](b)}g[h]?g[h](b,e,false):g.attachEvent("on"+b,e);c.P(1);var d=j.createElement("script"),m=document.getElementsByTagName("script")[0];d.type="text/javascript";d.async=true;d.src=a+"//"+c.l;m.parentNode.insertBefore(d,m);c.P(2)})()})({loader:(function(a){return "static.olark.com/jsclient/loader1.js?ts="+(a?a[1]:(+new Date))})(document.cookie.match(/olarkld=([0-9]+)/)),name:"olark",methods:["configure","extend","declare","identify"]}); olark.identify('5923-676-10-4978');/*]]>*/</script>
  	<script src="http://code.jquery.com/jquery.js"></script>
  	<script src="/js/bootstrap.min.js"></script>
  </body>
</html>