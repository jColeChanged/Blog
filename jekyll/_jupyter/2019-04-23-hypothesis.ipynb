{
  "cells": [
    {
      "cell_type": "raw",
      "source": "---\nlayout: post\ntitle: Toward Better Tests\npublished: true\n---\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% raw\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fkxKeiGT_4ka",
        "pycharm": {}
      },
      "source": "{:.blockquote}\n\u003e It is not only the violin that shapes the violinist, we are all\nshaped by the tools we train ourselves to use, and in this respect\nprogramming languages have a devious influence: they shape our\nthinking habits.\n\u003e\n\u003e Edsger W. Dijkstra\n\nIn order to do testing well, it\u0027s important to have tools that enable good practice.\nThis post uses Python to introduce stochastic property testing, but the general idea \nis applicable beyond Python and even beyond programming. I hope in reading this you \nreceive or are inspired to find an instrument which will help you to build more reliable \nsoftware \\[1\\] \\[2\\]. \n\nWhat a test *does* is assert something about a program. Lets assume we \nhave a function that checks to see if the result of an addition function is always\npositive. While trying to do so, it could `assert` that the sum of `1` and `2` \nis greater than `0`."
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 876,
          "status": "ok",
          "timestamp": 1555030896809,
          "user": {
            "displayName": "Joshua Cole",
            "photoUrl": "https://lh3.googleusercontent.com/-azxup7Woe44/AAAAAAAAAAI/AAAAAAAAAfs/oqkqz6uDXqw/s64/photo.jpg",
            "userId": "00001493690827580996"
          },
          "user_tz": 420
        },
        "id": "24PM35zG_4kb",
        "outputId": "e613c09b-6cf9-47c9-ec9d-d382a07a7ff0",
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The test passed!\n"
          ]
        }
      ],
      "source": "def add(a, b):\n    return a + b\n\n# Unparameterized test\n\ndef test_sum_positive():\n   assert add(1, 2) \u003e 0\n\ntry:\n    test_sum_positive()\n    print(\"The test passed.\")\nexcept AssertionError:\n    print(\"The test failed.\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O9jluLrJ_4kh",
        "pycharm": {}
      },
      "source": [
        "What a test *is* is a measurement of program execution under specific\n",
        "conditions. Since a test is a measurement, it follows that it is a [statistic](https://en.wikipedia.org/wiki/Statistic). So reallly, every set of tests is \n",
        "a sampling from the population of possible program executions.\n",
        "\n",
        "Seeing tests through the lens of statistics is useful, because one of the things \n",
        "statistics does is draw attention to the importance of good sampling. A hardcoded \n",
        "unit test might not seem like a problem, since any test at all is better than none.\n",
        "Statistics reminds us that \n",
        "[this type of sampling is flawed](https://en.wikipedia.org/wiki/Sampling_bias) \n",
        "such that we can\u0027t necessarily generalize from the sample population to the actual \n",
        "population. \n",
        "\n",
        "The measurement of `add(1, 2)` does not give much confidence that the addition \n",
        "function always returns positive results. The sampling of the add function isn\u0027t \n",
        "even close to being representative of the population of possible program executions.\n",
        "\n",
        "For the single addition test above, the test would pass. Despite this, there are\n",
        "many possible ways that the addition function could be called so as to fail to \n",
        "produce a positive number. The most obvious cases are when it\u0027s arguments aren\u0027t \n",
        "positive numbers, but there are others [3].\n",
        "\n",
        "One thing we know from statistics is that larger sample sizes are better than \n",
        "smaller sample sizes. Can we write our tests to include more samples, to help \n",
        "make our sample more represenative of all the possible ways the program could \n",
        "be executed? \n",
        "\n",
        "We can. It is possible to write a hundred variants of the add testing function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 1050,
          "status": "ok",
          "timestamp": 1555030896997,
          "user": {
            "displayName": "Joshua Cole",
            "photoUrl": "https://lh3.googleusercontent.com/-azxup7Woe44/AAAAAAAAAAI/AAAAAAAAAfs/oqkqz6uDXqw/s64/photo.jpg",
            "userId": "00001493690827580996"
          },
          "user_tz": 420
        },
        "id": "JE0Gc3HX_4ki",
        "outputId": "7cc1d6d8-69a2-404f-f1b4-451800d4b6a7",
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The tests passed!\n"
          ]
        }
      ],
      "source": "def add(a, b):\n    return a + b\n\n# An example of a bunch of unparameterized tests\n\nskip \u003d 3\na_few \u003d 99\n\ndef test_sum_positive_1():\n   assert add(1, 2) \u003e 0\n\ndef test_sum_positive_2():\n   assert add(skip, a_few) \u003e 0\n\n# ... snip\n\ndef test_sum_positive_3():\n   assert add(99, 100) \u003e 0\n\ntry:\n    test_sum_positive_1()\n    test_sum_positive_2()\n    # ... snip\n    test_sum_positive_3()\n    print(\"The tests passed!\")\nexcept AssertionError:\n    print(\"A test failed.\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Wnv96CW-_4kl",
        "pycharm": {}
      },
      "source": [
        "Even though we can do this, we shouldn\u0027t.\n",
        "\n",
        "Most functions take arguments. Even basic functions like the addition function\n",
        "can take arguments: the numbers to add together. Test functions can take\n",
        "arguments just like other functions can. Instead of hard coding `1` and `2` a\n",
        "test could be written which asserts that `a` plus `b` is positive.\n",
        "\n",
        "This type of test, a test which accepts arguments, is called a parameterized\n",
        "test. An advantage that a parameterized test has over an unparameterized test\n",
        "is that by passing a parameterized test different sets of arguments, it\u0027s possible \n",
        "to more concisely measure our program execution properties."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 1037,
          "status": "ok",
          "timestamp": 1555030896997,
          "user": {
            "displayName": "Joshua Cole",
            "photoUrl": "https://lh3.googleusercontent.com/-azxup7Woe44/AAAAAAAAAAI/AAAAAAAAAfs/oqkqz6uDXqw/s64/photo.jpg",
            "userId": "00001493690827580996"
          },
          "user_tz": 420
        },
        "id": "vzcOcAdq_4km",
        "outputId": "94dbdf49-077f-49ce-ebda-f19ab14f25a6",
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The tests passed!\n"
          ]
        }
      ],
      "source": "def add(a, b):\n    return a + b\n\nskip \u003d 3\na_few \u003d 99\n\n# An example of a bunch of tests, using paramaterized tests\n\ndef test_paramaterized_sum_positive(a, b):\n    assert add(a, b) \u003e 0\n\ndef paramaterized_test_runner():   \n    test_paramaterized_sum_positive(1, 2)\n    test_paramaterized_sum_positive(skip, a_few)\n    # ... snip\n    test_paramaterized_sum_positive(99, 100)\n\ntry:\n    paramaterized_test_runner()\n    print(\"The tests passed!\")\nexcept AssertionError:\n    print(\"A test failed.\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ftuDomP__4kp",
        "pycharm": {}
      },
      "source": [
        "In the case of the sum testing function, this approach only saves about a line per\n",
        "measurement, because the test is relatively small. There are many tests which\n",
        "are longer than this. So consider a testing function that had ten lines of\n",
        "code, instead of only two.\n",
        "\n",
        "With an unparameterized test, if we were to take one hundred samples from the\n",
        "space of possible program executions, we would need to introduce a thousand\n",
        "lines worth of potential errors. Meanwhile, in the case of parameterized tests,\n",
        "we only need to risk about a hundred and ten lines worth of potential\n",
        "errors. This savings of nearly nine hundred potential sources of error is\n",
        "considerable. By this metric, parameterized tests are exceedingly better than \n",
        "unparameterized tests.\n",
        "\n",
        "| Function Length | Argument Length  | # Tests | Lines for Paramterized Tests | Lines for Unparamterized Tests|\n",
        "|-----------------|------------------|---------|------------------------------|-------------------------------|\n",
        "| 2               | 1                | 100     | 102                          | 200                           |\n",
        "| 10              | 1                | 100     | 110                          | 1000                          |\n",
        "\n",
        "Parameterized tests are supported in most\n",
        "popular testing frameworks. For example, `pytest` provides a higher-order\n",
        "function that accepts argument sets and a parameterized test and returns a test\n",
        "which tests every test variation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SEZZZdkI_4kp",
        "pycharm": {}
      },
      "source": "## [Installing Pytest](#installing-pytest)\n\nBefore I can show what parameterized tests look like in `pytest`, we need to install `pytest` and set it up \nto work in an ipython notebook.\n\nFirst, find the Python installation we\u0027re using to run Jupyter."
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 1024,
          "status": "ok",
          "timestamp": 1555030896999,
          "user": {
            "displayName": "Joshua Cole",
            "photoUrl": "https://lh3.googleusercontent.com/-azxup7Woe44/AAAAAAAAAAI/AAAAAAAAAfs/oqkqz6uDXqw/s64/photo.jpg",
            "userId": "00001493690827580996"
          },
          "user_tz": 420
        },
        "id": "4y4lB6w9_4kq",
        "outputId": "c87d473d-e797-4c71-a0ce-59013b2c8bda",
        "pycharm": {}
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\u0027/usr/bin/python3\u0027"
            ]
          },
          "execution_count": 4,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import sys\n",
        "sys.executable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "81y-Cq_V_4kt",
        "pycharm": {}
      },
      "source": "Next, install the dependencies we need into the environment."
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 4357,
          "status": "ok",
          "timestamp": 1555030900343,
          "user": {
            "displayName": "Joshua Cole",
            "photoUrl": "https://lh3.googleusercontent.com/-azxup7Woe44/AAAAAAAAAAI/AAAAAAAAAfs/oqkqz6uDXqw/s64/photo.jpg",
            "userId": "00001493690827580996"
          },
          "user_tz": 420
        },
        "id": "zEEkCDAd_4ku",
        "outputId": "fde80f71-c24e-4465-9847-cf0b76d52568",
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (3.6.4)\n",
            "Requirement already satisfied: ipython-pytest in /usr/local/lib/python3.6/dist-packages (0.0.1)\n",
            "Requirement already satisfied: atomicwrites\u003e\u003d1.0 in /usr/local/lib/python3.6/dist-packages (from pytest) (1.3.0)\n",
            "Requirement already satisfied: attrs\u003e\u003d17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest) (19.1.0)\n",
            "Requirement already satisfied: pluggy\u003c0.8,\u003e\u003d0.5 in /usr/local/lib/python3.6/dist-packages (from pytest) (0.7.1)\n",
            "Requirement already satisfied: more-itertools\u003e\u003d4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest) (7.0.0)\n",
            "Requirement already satisfied: six\u003e\u003d1.10.0 in /usr/local/lib/python3.6/dist-packages (from pytest) (1.11.0)\n",
            "Requirement already satisfied: py\u003e\u003d1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest) (1.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest) (40.9.0)\n"
          ]
        }
      ],
      "source": [
        "! /usr/bin/python3 -m pip install pytest ipython-pytest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JDdJcq_e_4ky",
        "pycharm": {}
      },
      "source": "Finally, tell ipython to load the `ipython_pytest` extension."
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "TvJqWsyk_4kz",
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "%load_ext ipython_pytest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Wjjh1RuD_4k2",
        "pycharm": {}
      },
      "source": "Now with `pytest` installed, we can use it to write some parameterized tests."
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 6130,
          "status": "ok",
          "timestamp": 1555030902140,
          "user": {
            "displayName": "Joshua Cole",
            "photoUrl": "https://lh3.googleusercontent.com/-azxup7Woe44/AAAAAAAAAAI/AAAAAAAAAfs/oqkqz6uDXqw/s64/photo.jpg",
            "userId": "00001493690827580996"
          },
          "user_tz": 420
        },
        "id": "8diqDIF6_4k2",
        "outputId": "59654f46-98da-43ce-a1fd-5debcd2e8981",
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d test session starts \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n",
            "platform linux -- Python 3.6.7, pytest-3.6.4, py-1.8.0, pluggy-0.7.1\n",
            "hypothesis profile \u0027default\u0027 -\u003e database\u003dDirectoryBasedExampleDatabase(\u0027/tmp/tmpuliw3v4m/.hypothesis/examples\u0027)\n",
            "rootdir: /tmp/tmpuliw3v4m, inifile:\n",
            "plugins: hypothesis-4.15.0\n",
            "collected 3 items\n",
            "\n",
            "_ipytesttmp.py ...                                                       [100%]\n",
            "\n",
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d 3 passed in 0.09 seconds \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n"
          ]
        }
      ],
      "source": [
        "%%pytest\n",
        "\n",
        "# Parameterized test, with pytest\n",
        "import pytest\n",
        "\n",
        "\n",
        "def add(a, b):\n",
        "    return a + b\n",
        "\n",
        "skip \u003d 3\n",
        "a_few \u003d 99\n",
        "\n",
        "\n",
        "@pytest.mark.parametrize(\u0027a, b\u0027, [\n",
        "  (1, 2),\n",
        "  (skip, a_few),\n",
        "  # ...\n",
        "  (99, 100),\n",
        "])\n",
        "def test_sum_positive(a, b):\n",
        "   assert add(a, b) \u003e 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "66ExMFXO_4k5",
        "pycharm": {}
      },
      "source": [
        "Just with this, we\u0027re already in a much better place than we were. We\u0027re now testing multiple different parameterizations of our program. However, one of the only reasons this seems reasonable is that all this time the test specification has been skipping large numbers of parameterizations in the interest of brevity. In a real program, no one wants to have to type out a hundred different hard coded test cases. What would be much better is to both have many different parameterizations and to also have brevity at the same time.\n",
        "\n",
        "One way to do that is to have our arguments be programmatically generated rather than specified by hand."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 6880,
          "status": "ok",
          "timestamp": 1555030902899,
          "user": {
            "displayName": "Joshua Cole",
            "photoUrl": "https://lh3.googleusercontent.com/-azxup7Woe44/AAAAAAAAAAI/AAAAAAAAAfs/oqkqz6uDXqw/s64/photo.jpg",
            "userId": "00001493690827580996"
          },
          "user_tz": 420
        },
        "id": "Wm31aaks_4k6",
        "outputId": "1fc22038-f1da-4d46-af8e-78ed85717024",
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d test session starts \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n",
            "platform linux -- Python 3.6.7, pytest-3.6.4, py-1.8.0, pluggy-0.7.1\n",
            "hypothesis profile \u0027default\u0027 -\u003e database\u003dDirectoryBasedExampleDatabase(\u0027/tmp/tmpuliw3v4m/.hypothesis/examples\u0027)\n",
            "rootdir: /tmp/tmp893idtid, inifile:\n",
            "plugins: hypothesis-4.15.0\n",
            "collected 98 items\n",
            "\n",
            "_ipytesttmp.py ......................................................... [ 58%]\n",
            ".........................................                                [100%]\n",
            "\n",
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d warnings summary \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n",
            "\u003cundetermined location\u003e\n",
            "  Module already imported so cannot be rewritten: hypothesis\n",
            "\n",
            "-- Docs: http://doc.pytest.org/en/latest/warnings.html\n",
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d 98 passed, 1 warnings in 0.57 seconds \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n"
          ]
        }
      ],
      "source": [
        "%%pytest\n",
        "\n",
        "# Parameterized test, with pytest\n",
        "import pytest\n",
        "\n",
        "\n",
        "def add(a, b):\n",
        "    return a + b\n",
        "\n",
        "def create_sum_arguments(start, end):\n",
        "   \"Generate a list of test parameterizations.\"\n",
        "   return [(a, a+1) for a in range(start, end)]\n",
        "\n",
        "@pytest.mark.parametrize(\u0027a, b\u0027, create_sum_arguments(1, 99))\n",
        "def test_sum_positive(a, b):\n",
        "   assert add(a, b) \u003e 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "U1z467Y9_4k8",
        "pycharm": {}
      },
      "source": "Let\u0027s take a moment to [compare](https://en.wikipedia.org/wiki/Big_O_notation) \ngenerating a list of argument with our previous approach of writing \nout each argument individually in terms of how many lines it takes to \nwrite the tests.\n\n|   Test Paramterizations       | Lines for hand specification | Lines for programmatic creation\n|-------------------------------|------------------------------|----------------\n|1                              | 1                            | 2\n|2                              | 2                            | 2\n|3                              | 3                            | 2\n|100                            | 100                          | 2\n|1000                           | 1000                         | 2\n|10000                          | 10000                        | 2\n|n                              | O(n)                         | O(1)\n\n\nWith list creation, we can list a hundred different \narguments in roughly two lines. With hand specification, it takes around a hundred lines.\nSo the number of lines of code per test with list creation grows O(1) while the number \nof lines of code per test with hand specification grows O(n) where n is the number of test cases.\n\nThis is a great improvement, especially if the constant size of each argument is high, but \neven with the add function **generating a million test paramterizations programmatically is \ncheaper in terms of hand movement than writing three manually**.\n\nWith that power in mind, one weakness of creating a list of test cases is the amount of memory used. It isn\u0027t obvious \nwith the add function, but consider the case of testing an image parsing library\nwith each test case being an image buffer of a 1 MB image.\n\nAlthough we are able to generate \nour test cases in O(1) lines of code, it is still going to consume O(n) memory where n is the \nnumber of images. If we generated a hundred samples, we would need 100 MB of space. With a \nmillion, we\u0027re likely to run out of memory and crash.\n\nIt is easy to fix that, by taking advantage of the [lazy evaluation](https://en.wikipedia.org/wiki/Lazy_evaluation) via a [generator expression](https://www.python.org/dev/peps/pep-0289/)."
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 7495,
          "status": "ok",
          "timestamp": 1555030903519,
          "user": {
            "displayName": "Joshua Cole",
            "photoUrl": "https://lh3.googleusercontent.com/-azxup7Woe44/AAAAAAAAAAI/AAAAAAAAAfs/oqkqz6uDXqw/s64/photo.jpg",
            "userId": "00001493690827580996"
          },
          "user_tz": 420
        },
        "id": "bn6MvVrx_4k8",
        "outputId": "2add54d0-d6a7-43b8-ea64-6b11644e9602",
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d test session starts \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n",
            "platform linux -- Python 3.6.7, pytest-3.6.4, py-1.8.0, pluggy-0.7.1\n",
            "hypothesis profile \u0027default\u0027 -\u003e database\u003dDirectoryBasedExampleDatabase(\u0027/tmp/tmpuliw3v4m/.hypothesis/examples\u0027)\n",
            "rootdir: /tmp/tmpzpc558np, inifile:\n",
            "plugins: hypothesis-4.15.0\n",
            "collected 98 items\n",
            "\n",
            "_ipytesttmp.py ......................................................... [ 58%]\n",
            ".........................................                                [100%]\n",
            "\n",
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d warnings summary \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n",
            "\u003cundetermined location\u003e\n",
            "  Module already imported so cannot be rewritten: hypothesis\n",
            "\n",
            "-- Docs: http://doc.pytest.org/en/latest/warnings.html\n",
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d 98 passed, 1 warnings in 0.56 seconds \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n"
          ]
        }
      ],
      "source": [
        "%%pytest\n",
        "\n",
        "# Parameterized test, with pytest\n",
        "import pytest\n",
        "\n",
        "\n",
        "def add(a, b):\n",
        "    return a + b\n",
        "\n",
        "def yield_sum_arguments(start, end):\n",
        "   \"Return a generator which yields test parameterizations tuples.\"\n",
        "   return ((a, a+1) for a in range(start, end))\n",
        "\n",
        "@pytest.mark.parametrize(\u0027a, b\u0027, yield_sum_arguments(1, 99))\n",
        "def test_sum_positive(a, b):\n",
        "   assert add(a, b) \u003e 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1ezr_LNb_4k_",
        "pycharm": {}
      },
      "source": "|   Test Paramterizations       | Memory used for list creation | Memory used in lazy evaluation\n|-------------------------------|-------------------------------|----------\n|1                              | 1 MB                          | 1 MB\n|2                              | 2 MB                          | 1 MB\n|3                              | 3 MB                          | 1 MB\n|100                            | 100 MB                        | 1 MB\n|1000                           | 1000 MB                       | 1 MB\n|10000                          | 10000 MB                      | 1 MB\n|n                              | O(n)                          | O(1)\n\nLazy evaluation helps with more than our space issue. It also improves the \ndevelopment cycle of an engineer interacting with the tests. When fixing breaking \ntest cases, we\u0027re not interested in every test. Having to do a lot of work \ngenerating test cases that will never run is a waste of time.\n\nIt\u0027s a good sign that as we improve the architecture of our tests, we start \ngetting more and more useful properties. Parameterized tests which are generated \nas needed are a step in the right direction.\n\nStill, so far none of the tests have managed to show us the obvious - calling \nadd on two negative numbers isn\u0027t going to return a positive number.\n\nReally, we don\u0027t want to write a loop that generates parameters so much as a loop that\ngenerates a set of parameters which finds places where our hypothesis about how\nour program works is not true.\n\nWe want something like:"
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ugRUrzfs_4lA",
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "# @pytest.mark.parametrize(\u0027a, b\u0027, a_scientific_search_strategy)\n",
        "# def test_sum_positive(a, b):\n",
        "#   assert add(a, b) \u003e 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1ZCsepy6_4lB",
        "pycharm": {}
      },
      "source": "\nHow can we do that?\n\nBack in the 1990s Koen Claessen and John Hughes\nwrote a paper on [QuickCheck](http://www.eecs.northwestern.edu/~robby/courses/395-495-2009-fall/quick.pdf), a Haskell testing library which aided Haskell\nprogrammers in formulating and testing the properties of programs [4]. The\nlibrary gives tools to make writing these sort of parameterized tests easy. It\neven goes farther than that and tries to simplify any samples it finds which\ncause a failure, so that the failures it finds are easier for an engineer to\nunderstand.\n\nIn the world of people who care about testing, this library has been influential.\nIt\u0027s been [ported to several other languages](https://hypothesis.works/articles/quickcheck-in-every-language/), with varying degrees of rigour. For some languages, it\u0027s even been elevated \ninto core testing libraries.\n\nHere is the instrument that I hope to give you.\nIt is a testing library, inspired by QuickCheck, called `hypothesis`."
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "n5yOa635_4lC",
        "pycharm": {}
      },
      "source": "\n## [What is hypothesis?](#what-is-hypothesis)\n\n{:.blockquote}\n\u003e Hypothesis is a modern implementation of property based testing.\n\u003e\n\u003e Hypothesis runs your tests against a much wider range of scenarios than a human\ntester could, finding edge cases in your code that you would otherwise have\nmissed. It then turns them into simple and easy to understand failures that\nsave you time and money compared to fixing them if they slipped through the\ncracks and a user had run into them instead.\n\u003e\n\u003e Hypothesis integrates into your normal testing workflow. Getting started is as\nsimple as installing a library and writing some code using it - no new services\nto run, no new test runners to learn.\n\u003e\n\u003e [Source](https://hypothesis.works)\n\n\n## [Installing Hypothesis](#installing-hypothesis)\n\nLet\u0027s begin with hypothesis where we left off with a paramterized testing strategy. To do that, we will first need to install it."
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 10538,
          "status": "ok",
          "timestamp": 1555030906578,
          "user": {
            "displayName": "Joshua Cole",
            "photoUrl": "https://lh3.googleusercontent.com/-azxup7Woe44/AAAAAAAAAAI/AAAAAAAAAfs/oqkqz6uDXqw/s64/photo.jpg",
            "userId": "00001493690827580996"
          },
          "user_tz": 420
        },
        "id": "AlqcndQt_4lD",
        "outputId": "46342da3-c2a5-46e4-b0bc-d6f99f93e2e1",
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: hypothesis in /usr/local/lib/python3.6/dist-packages (4.15.0)\n",
            "Requirement already satisfied: attrs\u003e\u003d16.0.0 in /usr/local/lib/python3.6/dist-packages (from hypothesis) (19.1.0)\n"
          ]
        }
      ],
      "source": [
        "! /usr/bin/python3 -m pip install hypothesis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "H8643yDN_4lH",
        "pycharm": {}
      },
      "source": [
        "Now that we\u0027ve installed it, we can write out our more scientific search strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 11156,
          "status": "ok",
          "timestamp": 1555030907203,
          "user": {
            "displayName": "Joshua Cole",
            "photoUrl": "https://lh3.googleusercontent.com/-azxup7Woe44/AAAAAAAAAAI/AAAAAAAAAfs/oqkqz6uDXqw/s64/photo.jpg",
            "userId": "00001493690827580996"
          },
          "user_tz": 420
        },
        "id": "V7Z4vkQT_4lI",
        "outputId": "abe38fb4-5fc1-432c-e22a-c27e7c110887",
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d test session starts \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n",
            "platform linux -- Python 3.6.7, pytest-3.6.4, py-1.8.0, pluggy-0.7.1\n",
            "hypothesis profile \u0027default\u0027 -\u003e database\u003dDirectoryBasedExampleDatabase(\u0027/tmp/tmpuliw3v4m/.hypothesis/examples\u0027)\n",
            "rootdir: /tmp/tmpxykp99zo, inifile:\n",
            "plugins: hypothesis-4.15.0\n",
            "collected 1 item\n",
            "\n",
            "_ipytesttmp.py F                                                         [100%]\n",
            "\n",
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d FAILURES \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n",
            "______________________________ test_sum_positive _______________________________\n",
            "\n",
            "    @given(strategies.integers(), strategies.integers())\n",
            "\u003e   def test_sum_positive(a, b):\n",
            "\n",
            "_ipytesttmp.py:10: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "a \u003d 0, b \u003d 0\n",
            "\n",
            "    @given(strategies.integers(), strategies.integers())\n",
            "    def test_sum_positive(a, b):\n",
            "\u003e      assert add(a, b) \u003e 0\n",
            "E      assert 0 \u003e 0\n",
            "E       +  where 0 \u003d add(0, 0)\n",
            "\n",
            "_ipytesttmp.py:11: AssertionError\n",
            "---------------------------------- Hypothesis ----------------------------------\n",
            "Falsifying example: test_sum_positive(a\u003d0, b\u003d0)\n",
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d warnings summary \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n",
            "\u003cundetermined location\u003e\n",
            "  Module already imported so cannot be rewritten: hypothesis\n",
            "\n",
            "-- Docs: http://doc.pytest.org/en/latest/warnings.html\n",
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d 1 failed, 1 warnings in 0.33 seconds \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n"
          ]
        }
      ],
      "source": [
        "%%pytest\n",
        "# Parameterized test, with pytest and hypothesis\n",
        "from hypothesis import given, strategies\n",
        "\n",
        "\n",
        "def add(a, b):\n",
        "    return a + b\n",
        "\n",
        "\n",
        "@given(strategies.integers(), strategies.integers())\n",
        "def test_sum_positive(a, b):\n",
        "   assert add(a, b) \u003e 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_o3f3bn8_4lL",
        "pycharm": {}
      },
      "source": "This simple, unassuming bit of code is doing a lot. It generates a hundred\nsamples; runs a hundred tests. Yet it does this with less code than our\nearlier parameterization approach and not only that, but for the first time we\nhave code which will quickly find the error that is so blatant. It discovers the \ncounterexamples where `a` and `b` are not positive.\n\nLet\u0027s break this down. `@given` is a [decorator](https://www.python.org/dev/peps/pep-0318/). \nA decorator is Python syntactic sugar for a [higher-order function](https://en.wikipedia.org/wiki/Higher-order_function), \nwhich is a function that accepts a function as an argument and returns another function. \nThe `given` decorator takes in a data generation strategies and a parameterized test and\nreturns a test which will call the parameterized test using the provided data\ngeneration strategy.\n\n`strategies` is a module imported from the `hypothesis` library. It contains\nfunctions which help an engineer to create samples from many common data types.\nIt also provides ways to combine and compose strategies, so that even \ncomplex data generation is possible.\n\n`integers` is one of the strategies it provides. It has\na few arguments it accept which can be used to change what \nintegers it will generate during testing, but I won\u0027t go into that here. The hypothesis project\nhas [excellent documentation](https://hypothesis.readthedocs.io/en/latest/data.html).\nYou can read about `integers()` and other data generation strategies there. For\nour purposes, it\u0027s more important to to know that there is a way to generate a lot of\ndifferent data. Even complex data.\n\nEarlier I showed how to generate integers. We could easily extend this to also\ngenerate floats, decimals, fractions, and complex numbers."
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 11334,
          "status": "ok",
          "timestamp": 1555030907390,
          "user": {
            "displayName": "Joshua Cole",
            "photoUrl": "https://lh3.googleusercontent.com/-azxup7Woe44/AAAAAAAAAAI/AAAAAAAAAfs/oqkqz6uDXqw/s64/photo.jpg",
            "userId": "00001493690827580996"
          },
          "user_tz": 420
        },
        "id": "diLyYkKJ_4lM",
        "outputId": "64ffdacf-d83a-4d17-d2a2-7b0696f6063c",
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d test session starts \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n",
            "platform linux -- Python 3.6.7, pytest-3.6.4, py-1.8.0, pluggy-0.7.1\n",
            "hypothesis profile \u0027default\u0027 -\u003e database\u003dDirectoryBasedExampleDatabase(\u0027/tmp/tmpuliw3v4m/.hypothesis/examples\u0027)\n",
            "rootdir: /tmp/tmpiq6lujss, inifile:\n",
            "plugins: hypothesis-4.15.0\n",
            "collected 1 item\n",
            "\n",
            "_ipytesttmp.py F                                                         [100%]\n",
            "\n",
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d FAILURES \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n",
            "______________________________ test_sum_positive _______________________________\n",
            "\n",
            "    @given(number_strategy, number_strategy)\n",
            "\u003e   def test_sum_positive(a, b):\n",
            "\n",
            "_ipytesttmp.py:18: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "a \u003d 0.0, b \u003d 0.0\n",
            "\n",
            "    @given(number_strategy, number_strategy)\n",
            "    def test_sum_positive(a, b):\n",
            "\u003e      assert add(a, b) \u003e 0\n",
            "E      assert 0.0 \u003e 0\n",
            "E       +  where 0.0 \u003d add(0.0, 0.0)\n",
            "\n",
            "_ipytesttmp.py:19: AssertionError\n",
            "---------------------------------- Hypothesis ----------------------------------\n",
            "Falsifying example: test_sum_positive(a\u003d0.0, b\u003d0.0)\n",
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d warnings summary \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n",
            "\u003cundetermined location\u003e\n",
            "  Module already imported so cannot be rewritten: hypothesis\n",
            "\n",
            "-- Docs: http://doc.pytest.org/en/latest/warnings.html\n",
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d 1 failed, 1 warnings in 0.18 seconds \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n"
          ]
        }
      ],
      "source": [
        "%%pytest\n",
        "# More complex data generation strategies\n",
        "\n",
        "from hypothesis import given, strategies\n",
        "\n",
        "\n",
        "def add(a, b):\n",
        "    return a + b\n",
        "\n",
        "number_strategy \u003d (\n",
        "  strategies.floats() |\n",
        "  strategies.decimals() |\n",
        "  strategies.complex_numbers() |\n",
        "  strategies.fractions() |\n",
        "  strategies.integers() \n",
        ")\n",
        "\n",
        "@given(number_strategy, number_strategy)\n",
        "def test_sum_positive(a, b):\n",
        "   assert add(a, b) \u003e 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ijdz8jgK_4lS",
        "pycharm": {}
      },
      "source": "Hypothesis gives a lot of expressive power. It was easy to get a data \ngenerator that generated a much more rich set of data for testing. \n\nFor the purpose of driving this point home, let\u0027s suppose that for some reason \nour addition function wasn\u0027t supposed to ever be called with `0` as an argument \nor with a number that somehow became a `NaN`."
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 11925,
          "status": "ok",
          "timestamp": 1555030907998,
          "user": {
            "displayName": "Joshua Cole",
            "photoUrl": "https://lh3.googleusercontent.com/-azxup7Woe44/AAAAAAAAAAI/AAAAAAAAAfs/oqkqz6uDXqw/s64/photo.jpg",
            "userId": "00001493690827580996"
          },
          "user_tz": 420
        },
        "id": "UW-miFRs_4lT",
        "outputId": "61e88817-517f-45d7-fde1-e87f84aebe89",
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d test session starts \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n",
            "platform linux -- Python 3.6.7, pytest-3.6.4, py-1.8.0, pluggy-0.7.1\n",
            "hypothesis profile \u0027default\u0027 -\u003e database\u003dDirectoryBasedExampleDatabase(\u0027/tmp/tmpuliw3v4m/.hypothesis/examples\u0027)\n",
            "rootdir: /tmp/tmpdxsnwtw3, inifile:\n",
            "plugins: hypothesis-4.15.0\n",
            "collected 1 item\n",
            "\n",
            "_ipytesttmp.py F                                                         [100%]\n",
            "\n",
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d FAILURES \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n",
            "______________________________ test_sum_positive _______________________________\n",
            "\n",
            "    @given(number_strategy, number_strategy)\n",
            "\u003e   def test_sum_positive(a, b):\n",
            "\n",
            "_ipytesttmp.py:37: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "a \u003d 1, b \u003d -1\n",
            "\n",
            "    @given(number_strategy, number_strategy)\n",
            "    def test_sum_positive(a, b):\n",
            "        assume(a !\u003d 0 and b !\u003d 0 and not math.isnan(a) and not math.isnan(b))\n",
            "\u003e       assert add(a, b) \u003e 0\n",
            "E       assert 0 \u003e 0\n",
            "E        +  where 0 \u003d add(1, -1)\n",
            "\n",
            "_ipytesttmp.py:39: AssertionError\n",
            "---------------------------------- Hypothesis ----------------------------------\n",
            "Falsifying example: test_sum_positive(a\u003d1, b\u003d-1)\n",
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d warnings summary \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n",
            "\u003cundetermined location\u003e\n",
            "  Module already imported so cannot be rewritten: hypothesis\n",
            "\n",
            "-- Docs: http://doc.pytest.org/en/latest/warnings.html\n",
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d 1 failed, 1 warnings in 0.42 seconds \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n"
          ]
        }
      ],
      "source": [
        "%%pytest\n",
        "# More complex data generation strategies\n",
        "import math\n",
        "\n",
        "\n",
        "from hypothesis import given, strategies\n",
        "\n",
        "\n",
        "def add(a, b):\n",
        "    return a + b\n",
        "\n",
        "number_strategy \u003d (\n",
        "  strategies.floats() |\n",
        "  strategies.decimals() |\n",
        "  strategies.complex_numbers() |\n",
        "  strategies.fractions() |\n",
        "  strategies.integers() \n",
        ").filter(lambda x: x !\u003d 0 and not math.isnan(x))\n",
        "\n",
        "@given(number_strategy, number_strategy)\n",
        "def test_sum_positive(a, b):\n",
        "    assert add(a, b) \u003e 0\n",
        "\n",
        "\n",
        "# Or we could use assume\n",
        "\n",
        "from hypothesis import assume\n",
        "\n",
        "number_strategy \u003d (\n",
        "  strategies.floats() |\n",
        "  strategies.decimals() |\n",
        "  strategies.complex_numbers() |\n",
        "  strategies.fractions() |\n",
        "  strategies.integers() \n",
        ")\n",
        "\n",
        "@given(number_strategy, number_strategy)\n",
        "def test_sum_positive(a, b):\n",
        "    assume(a !\u003d 0 and b !\u003d 0 and not math.isnan(a) and not math.isnan(b))\n",
        "    assert add(a, b) \u003e 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O8aG-Brm_4lW",
        "pycharm": {}
      },
      "source": "If you\u0027ve been playing with these examples, you may have noticed that \nhypothesis has found breaking examples quite easily. Not just ones where the \nfunction returned a negative result, but actual errors. The add function isn\u0027t \noverloaded so as to support adding arbitray numeric types. If it wasn\u0027t an error \nyou expected to see, than you may begin to understand just how useful this \nsearch for falsifying examples is. It doesn\u0027t just give you confidence that \nyour code works. It can end up teaching you something you hadn\u0027t known.\n\nAt the same time, you might not have seen these errors. When hypothesis runs \ntests, it generates them stoachastically. It\u0027s possible for two different runs \nof hypothesis to generate different examples.\n\nThis is an important thing to keep in mind when using `hypothesis`. It is \ngenerating data, not doing magic. Under the hood it has a search strategy \nwhich is searching for things to pass in as test parameters. Given a \nstrict data generation strategy, it might not be able to find valid \nparameterizations. Even if it does find parameterizations, it could still \nskip over important test cases that an engineer knows to be of critical \nimportance.\n\nFor this reason and others, it can still be useful to hand specify important test cases. This can \nbe done using the `@example` decorator."
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 12137,
          "status": "ok",
          "timestamp": 1555030908217,
          "user": {
            "displayName": "Joshua Cole",
            "photoUrl": "https://lh3.googleusercontent.com/-azxup7Woe44/AAAAAAAAAAI/AAAAAAAAAfs/oqkqz6uDXqw/s64/photo.jpg",
            "userId": "00001493690827580996"
          },
          "user_tz": 420
        },
        "id": "bPDGNGjf_4lX",
        "outputId": "7d80141a-fafc-4683-c28c-9a29d419bb3b",
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d test session starts \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n",
            "platform linux -- Python 3.6.7, pytest-3.6.4, py-1.8.0, pluggy-0.7.1\n",
            "hypothesis profile \u0027default\u0027 -\u003e database\u003dDirectoryBasedExampleDatabase(\u0027/tmp/tmpuliw3v4m/.hypothesis/examples\u0027)\n",
            "rootdir: /tmp/tmpp9g8cudf, inifile:\n",
            "plugins: hypothesis-4.15.0\n",
            "collected 1 item\n",
            "\n",
            "_ipytesttmp.py F                                                         [100%]\n",
            "\n",
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d FAILURES \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n",
            "______________________________ test_sum_positive _______________________________\n",
            "\n",
            "    @given(strategies.integers(), strategies.integers())\n",
            "\u003e   @example(-100, -100)\n",
            "    def test_sum_positive(a, b):\n",
            "\n",
            "_ipytesttmp.py:10: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "/usr/local/lib/python3.6/dist-packages/hypothesis/core.py:327: in execute_explicit_examples\n",
            "    test_runner(None, lambda data: test(*arguments, **example_kwargs))\n",
            "/usr/local/lib/python3.6/dist-packages/hypothesis/executors.py:56: in default_new_style_executor\n",
            "    return function(data)\n",
            "/usr/local/lib/python3.6/dist-packages/hypothesis/core.py:327: in \u003clambda\u003e\n",
            "    test_runner(None, lambda data: test(*arguments, **example_kwargs))\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "a \u003d -100, b \u003d -100\n",
            "\n",
            "    @given(strategies.integers(), strategies.integers())\n",
            "    @example(-100, -100)\n",
            "    def test_sum_positive(a, b):\n",
            "\u003e      assert add(a, b) \u003e 0\n",
            "E      assert -200 \u003e 0\n",
            "E       +  where -200 \u003d add(-100, -100)\n",
            "\n",
            "_ipytesttmp.py:12: AssertionError\n",
            "---------------------------------- Hypothesis ----------------------------------\n",
            "Falsifying example: test_sum_positive(a\u003d-100, b\u003d-100)\n",
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d warnings summary \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n",
            "\u003cundetermined location\u003e\n",
            "  Module already imported so cannot be rewritten: hypothesis\n",
            "\n",
            "-- Docs: http://doc.pytest.org/en/latest/warnings.html\n",
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d 1 failed, 1 warnings in 0.20 seconds \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n"
          ]
        }
      ],
      "source": [
        "%%pytest\n",
        "# Parameterized test, with pytest and hypothesis\n",
        "from hypothesis import given, strategies, example\n",
        "\n",
        "\n",
        "def add(a, b):\n",
        "    return a + b\n",
        "\n",
        "\n",
        "@given(strategies.integers(), strategies.integers())\n",
        "@example(-100, -100)\n",
        "def test_sum_positive(a, b):\n",
        "   assert add(a, b) \u003e 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VmDFnvM-_4lZ",
        "pycharm": {}
      },
      "source": "## [Environmental Considerations](#environmental-considerations)\n\nThere are concerns that a developer has which aren\u0027t about the beauty of an \napproach, but about the practicality of use. There are many programming \nlanguages that are objectively better than Python by one metric or another, yet it has it\u0027s niche, \nbecause it chooses to be readable rather than fast or easily parsed by computers. \n\nSo far we\u0027ve discussed code. Let\u0027s move on to some other software engineering \nbest practices and see how `hypothesis` holds up.\n\nIronically, since we\u0027re using Python, when it comes to testing speed matters. \nThere are things that can be done when something takes seconds\nwhich can\u0027t even be contemplated when something takes an hour. For example, \nhaving a continual testing loop running concurrently with development or \nrunning tests before check in become a much simpler thing when tests run \nquickly, but can slow down development speed if tests take hours to run.\n\n`hypothesis` does well in helping to deliver the ideal of fast running tests. \nIt provides tools for limiting both the runtime of tests according to the clock \nand for limiting the runtime of tests in terms of the number of test cases it checks.\n\nOn the topic of a contiual test loop, it\u0027s also nice to keep a record of what tests \nfail and than re-run those tests on the next run of the tester. `hypothesis` does \nthis too.\n\nOne way that these sorts of things can be done on a per test basis is through the settings\ndecorator."
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 12860,
          "status": "ok",
          "timestamp": 1555030908950,
          "user": {
            "displayName": "Joshua Cole",
            "photoUrl": "https://lh3.googleusercontent.com/-azxup7Woe44/AAAAAAAAAAI/AAAAAAAAAfs/oqkqz6uDXqw/s64/photo.jpg",
            "userId": "00001493690827580996"
          },
          "user_tz": 420
        },
        "id": "WhcpgiIF_4lb",
        "outputId": "aee4d761-4db1-4733-b65f-0c657d5bf3be",
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d test session starts \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n",
            "platform linux -- Python 3.6.7, pytest-3.6.4, py-1.8.0, pluggy-0.7.1\n",
            "hypothesis profile \u0027default\u0027 -\u003e database\u003dDirectoryBasedExampleDatabase(\u0027/tmp/tmpuliw3v4m/.hypothesis/examples\u0027)\n",
            "rootdir: /tmp/tmp0nvvccac, inifile:\n",
            "plugins: hypothesis-4.15.0\n",
            "collected 3 items\n",
            "\n",
            "_ipytesttmp.py ...                                                       [100%]\n",
            "\n",
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d warnings summary \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n",
            "\u003cundetermined location\u003e\n",
            "  Module already imported so cannot be rewritten: hypothesis\n",
            "\n",
            "-- Docs: http://doc.pytest.org/en/latest/warnings.html\n",
            "\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d 3 passed, 1 warnings in 0.80 seconds \u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\n"
          ]
        }
      ],
      "source": [
        "%%pytest\n",
        "\n",
        "from hypothesis import given, settings, strategies\n",
        "\n",
        "@given(strategies.integers())\n",
        "@settings(max_examples\u003d50) # Default is 100\n",
        "def test_this_a_little(x):\n",
        "    assert True\n",
        "    \n",
        "\n",
        "@given(strategies.integers())\n",
        "def test_this_with_default_setttings(x):\n",
        "    assert True\n",
        "    \n",
        "@given(strategies.integers())\n",
        "@settings(max_examples\u003d500)\n",
        "def test_this_thoroughly(x):\n",
        "    assert True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LdoA5FHL_4ld",
        "pycharm": {}
      },
      "source": [
        "Another way is via the settings profile:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "colab_type": "code",
        "executionInfo": {
          "elapsed": 12845,
          "status": "ok",
          "timestamp": 1555030908951,
          "user": {
            "displayName": "Joshua Cole",
            "photoUrl": "https://lh3.googleusercontent.com/-azxup7Woe44/AAAAAAAAAAI/AAAAAAAAAfs/oqkqz6uDXqw/s64/photo.jpg",
            "userId": "00001493690827580996"
          },
          "user_tz": 420
        },
        "id": "TPSvW7DE_4le",
        "outputId": "ee6a8d69-d461-4a95-bdd5-882dd50c7a70",
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max examples before loading ci profile 100\n",
            "Max examples after loading ci profile 1000\n"
          ]
        }
      ],
      "source": [
        "from hypothesis import  settings, Verbosity\n",
        "\n",
        "settings.register_profile(\"ci\", max_examples\u003d1000)\n",
        "print(\"Max examples before loading ci profile\", settings().max_examples)\n",
        "settings.load_profile(\"ci\")\n",
        "print(\"Max examples after loading ci profile\", settings().max_examples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "v1ZmX16R_4lg",
        "pycharm": {}
      },
      "source": [
        "Beyond just running fast, thorough tests are also a nice to thing to have. \n",
        "Thorough testing gives more confidence that software works as desired than \n",
        "minimal testing, all other factors ignored.\n",
        "\n",
        "Since the settings profiles can be set based on flags, `hypothesis` tackles \n",
        "supporting more thorough testing in automated build servers in a straightforward \n",
        "way. When doing local development, we can run just a few tests. Meanwhile, \n",
        "on a build server we can run thousands."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "UeADqCAE_4lg",
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from hypothesis import settings, Verbosity\n",
        "settings.register_profile(\"weekend\", max_examples\u003d100000)\n",
        "settings.register_profile(\"nightly\", max_examples\u003d10000)\n",
        "settings.register_profile(\"ci\", max_examples\u003d1000)\n",
        "settings.register_profile(\"dev\", max_examples\u003d100)\n",
        "settings.register_profile(\"debug\", max_examples\u003d10, verbosity\u003dVerbosity.verbose)\n",
        "settings.load_profile(os.getenv(u\u0027HYPOTHESIS_PROFILE\u0027, \u0027default\u0027))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Og1xtUtK_4li",
        "pycharm": {}
      },
      "source": "The `sympy` project found errors after letting `hypothesis` generate millions of\nexamples. Errors that hadn\u0027t been found with less thorough testing. \n\nHypothesis gives us an easy to tune knob which results in more thorough testing. \nThis control gives a lot more power to engineers to make trade offs based on the \ncontext in which tests are being run. Out of band testing can be thorough to the point of\nabsurdity while work that is in flow can be fast so as to maximize engineer\nproductivity.\n\n## [Library integration](#library-integration)\n\nHypothesis has support for many popular python librarys, including django. It can infer strategy creation from a Django model or form. In a hypothetical example, let\u0027s say you have an email address model which is related to a contact model, which is related to an organization model. In order to get a strategy which could automatically fill in all the data for instances of those models all that would be needed would be:"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ei_zo0sg_4lp",
        "pycharm": {}
      },
      "source": "## In Summary\n\nCast aside the hard coded test cases and accept a\nbetter violin. [hypothesis.works](https://hypothesis.works/)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oUSP_NwG_4lq",
        "pycharm": {}
      },
      "source": "## [Footnotes](#footnotes)\n\n\\[1\\]: In his wonderful essay the [Humble Programmer](https://www.cs.utexas.edu/~EWD/transcriptions/EWD03xx/EWD340.html) Edsger W. Dijkstra goes on at length about limiting the scope of programs to those that can actually be contended with. In the same spirit of humility, I want to qualify my own words. Reliable software isn\u0027t easy to write. Great engineers and scientists have quipped that programming is the act of putting bugs into a program, since debugging is the act of taking them out. For this reason, when I write \"software\" I\u0027m defining it in a narrow sense of the word; not full applications, but small sections of a program.\n\nI\u0027m also defining \"reliable\" in a narrow sense. When I say reliable, I mean that we have good reason to be confident that the software has the properties which we desire it to have. We expect the software to work how we think it works, rather than some other way. So reliable as in \"we think we might be able rely on it\", not, this definitely works always. I wish I could say that I was doing more than building confidence that the programs we write really did have the properties we hoped they would have, but the instrument I\u0027m sharing does not generate proofs. It is a way to generate tests.\n\n\\[2\\]: This goes beyond programming languages and testing libraries. For example, using Powerpoint can damage thinking. One humorous example of PowerPoint struggling to capture importance is the [Gettysburg address as converted to powerpoint by Peter Norvig](https://norvig.com/Gettysburg/). \n\nA more serious discussion of the weaknesses and corrupting biases of mediums can be found in the book [Amusing Ourselves to Death: Public Discourse in the Age of Show Business](https://www.amazon.com/Amusing-Ourselves-Death-Discourse-Business/dp/014303653X) by Neil Postman. In the book, Neil argues persuasively that mediums have an influence on the things that are written in them and that the influence of the medium of a long text document is more conductive to good thinking than a primarly visual medium. The critical take away form the book is that choosing the wrong medium doesn\u0027t just change how the content is presented, it changes the content itself. Given this, it shouldn\u0027t be much of a suprise that companies like Amazon are [forsaking PowerPoint in favor of the written word](https://www.inc.com/carmine-gallo/jeff-bezos-bans-powerpoint-in-meetings-his-replacement-is-brilliant.html).\n\n\\[3\\]: Beyond the normal errors there are other problems the add function is potentially \nvulnerable to. For example, we haven\u0027t verified that it is resistant to bit corruption \nby alpha particles or faulty hardware. Everything we leave out of our test is a \npotential source of a flaky test, since each uncontrolled parameter is still an implicit \nparameters that can change program execution.\n\nIn the case of the add function, this problem might feel theoretical, but it\u0027s not \ntheoretical at all with remote APIs. Allowing tests to simulate failures is important."
    }
  ],
  "metadata": {
    "colab": {
      "name": "Copy of Testing with Hypothesis.ipynb",
      "provenance": [
        {
          "file_id": "197xpRo1cYGSm19r3K1mWCkywZk_ijp_q",
          "timestamp": 1555030052149
        }
      ],
      "version": "0.3.2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "stem_cell": {
      "cell_type": "raw",
      "source": "",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}